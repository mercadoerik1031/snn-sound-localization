{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XWQW4Zj-Cwt",
        "outputId": "fb8661e9-bee7-4cdb-d60f-7d57e452df71"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install snntorch --quiet"
      ],
      "metadata": {
        "id": "0b1hp-2BO2-W"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ogIzFacrhYZD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    # Filter Data\n",
        "    \"metadata_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/metadata.parquet\",\n",
        "    \"speech_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/ambisonics_lite\",\n",
        "    \"noise_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/noise_ambisonics_lite\",\n",
        "    \"is_lite_version\": True,\n",
        "\n",
        "    # Load Mix and STFT Audio\n",
        "    \"sr\": 16_000,\n",
        "    \"duration\": 0.5,\n",
        "    \"noise_ratio\": None, # Optional (Large #s make noise louder, Small #s make noise quieter)\n",
        "    \"n_fft\": 512,\n",
        "    \"hop_length\": 128,\n",
        "    \"win_length\": 512,\n",
        "\n",
        "    # Split Data\n",
        "    \"val_size\": 0.15,\n",
        "\n",
        "    # Dataset\n",
        "    \"num_steps\": 10,\n",
        "\n",
        "    # SNN\n",
        "    \"beta\": 0.65,\n",
        "}"
      ],
      "metadata": {
        "id": "nXnOrdx29edp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data(\n",
        "        metadata_pth=config[\"metadata_path\"],\n",
        "        speech_pth=config[\"speech_path\"],\n",
        "        noise_pth=config[\"noise_path\"],\n",
        "        lite_version=config[\"is_lite_version\"]\n",
        "):\n",
        "\n",
        "    metadata = pd.read_parquet(metadata_pth, engine=\"pyarrow\")\n",
        "    if lite_version:\n",
        "        metadata = metadata[metadata[\"lite_version\"] == True]\n",
        "\n",
        "    data = []\n",
        "    for _, row in metadata.iterrows():\n",
        "        sample = {\n",
        "            \"sample_id\": row[\"sample_id\"],\n",
        "            \"speech_path\": os.path.join(speech_pth, f\"{row['sample_id']:06}.flac\"),\n",
        "            \"noise_path\": os.path.join(noise_pth, f\"{row['sample_id']:06}.flac\") if noise_pth else None,\n",
        "            \"azimuth\": row[\"speech/azimuth\"],\n",
        "            \"elevation\": row[\"speech/elevation\"],\n",
        "            \"split\": row[\"split\"]\n",
        "        }\n",
        "        data.append(sample)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "T-yb90H8ry_W"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, val_size=config[\"val_size\"]):\n",
        "    train_data = [sample for sample in data if sample[\"split\"] == \"train\"]\n",
        "    test_data = [sample for sample in data if sample[\"split\"] == \"test\"]\n",
        "\n",
        "    train_data, val_data = train_test_split(train_data, test_size=val_size, random_state=42)\n",
        "\n",
        "    return train_data, val_data, test_data\n"
      ],
      "metadata": {
        "id": "KzoFwpF4WMFP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mix_and_stft_foa_audio(\n",
        "        speech_pth,\n",
        "        noise_pth=None,\n",
        "        sr=config[\"sr\"],\n",
        "        duration=config[\"duration\"],\n",
        "        noise_ratio=config[\"noise_ratio\"],\n",
        "        n_fft=config[\"n_fft\"],\n",
        "        hop_length=config[\"hop_length\"],\n",
        "        win_length=config[\"win_length\"]\n",
        "):\n",
        "    # Function to load, resample, normalize, and trim/pad audio\n",
        "    def preprocess_audio(audio_pth, sr, target_len):\n",
        "        audio, audio_sr = torchaudio.load(audio_pth)\n",
        "        if audio_sr != sr:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=audio_sr, new_freq=sr)\n",
        "            audio = resample_transform(audio)\n",
        "        max_val = audio.abs().max()\n",
        "        if max_val > 0:  # Avoid division by zero\n",
        "            audio = audio / max_val\n",
        "        if audio.size(1) > target_len:\n",
        "            audio = audio[:, :target_len]\n",
        "        elif audio.size(1) < target_len:\n",
        "            padding_size = target_len - audio.size(1)\n",
        "            audio = torch.nn.functional.pad(audio, (0, padding_size), \"constant\", 0)\n",
        "        return audio\n",
        "\n",
        "    target_len = int(duration * sr)\n",
        "    speech_audio = preprocess_audio(speech_pth, sr, target_len)\n",
        "    should_renormalize = False\n",
        "\n",
        "    if noise_pth is not None:\n",
        "        noise_audio = preprocess_audio(noise_pth, sr, target_len)\n",
        "\n",
        "        if noise_ratio is not None:\n",
        "            # Adjust noise level relative to speech\n",
        "            noise_audio = noise_audio * noise_ratio\n",
        "            should_renormalize = True\n",
        "\n",
        "        # Mix speech and noise\n",
        "        mixed_audio = speech_audio + noise_audio\n",
        "    else:\n",
        "        mixed_audio = speech_audio\n",
        "\n",
        "    if should_renormalize:\n",
        "        # Re-normalize only if noise has been adjusted and mixed\n",
        "        max_val = mixed_audio.abs().max()\n",
        "        if max_val > 0:\n",
        "            mixed_audio = mixed_audio / max_val\n",
        "\n",
        "    # Compute the STFT of the mixed audio\n",
        "    stft = torch.stft(mixed_audio,\n",
        "                      n_fft=n_fft,\n",
        "                      hop_length=hop_length,\n",
        "                      win_length=win_length,\n",
        "                      window=torch.hann_window(win_length),\n",
        "                      center=True,\n",
        "                      normalized=False,\n",
        "                      onesided=True,\n",
        "                      return_complex=True)\n",
        "\n",
        "    return stft\n"
      ],
      "metadata": {
        "id": "gKUl7kz-uRUU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_active_reactive_intensities(stft, rho=1.21, c=343):\n",
        "    \"\"\"\n",
        "    Compute active and reactive intensity vectors from STFT of 4-channel FOA audio.\n",
        "    Args:\n",
        "    - stft: STFT of the FOA audio with shape [4, Frequency Bins, Time Frames].\n",
        "    - rho: Mean density of air (in kg/m^3).\n",
        "    - c: Speed of sound in air (in m/s).\n",
        "\n",
        "    Returns:\n",
        "    - Ia: Active intensity vector.\n",
        "    - Ir: Reactive intensity vector.\n",
        "    \"\"\"\n",
        "    # Constants\n",
        "    three = torch.tensor(3.0, dtype=torch.float, device=stft.device)\n",
        "    normalization_factor = -1 / (rho * c * torch.sqrt(three))\n",
        "\n",
        "    # Extract channels\n",
        "    p = stft[0]  # Pressure (W channel)\n",
        "    vx = stft[1] * normalization_factor  # Velocity X\n",
        "    vy = stft[2] * normalization_factor  # Velocity Y\n",
        "    vz = stft[3] * normalization_factor  # Velocity Z\n",
        "\n",
        "    # Compute complex conjugate of pressure\n",
        "    p_star = torch.conj(p)\n",
        "\n",
        "    # Calculate active and reactive intensity vectors\n",
        "    Ia_x = torch.real(p_star * vx)\n",
        "    Ia_y = torch.real(p_star * vy)\n",
        "    Ia_z = torch.real(p_star * vz)\n",
        "\n",
        "    Ir_x = torch.imag(p_star * vx)\n",
        "    Ir_y = torch.imag(p_star * vy)\n",
        "    Ir_z = torch.imag(p_star * vz)\n",
        "\n",
        "    # Summing up the components to get total active and reactive intensities\n",
        "    Ia = Ia_x + Ia_y + Ia_z  # Total active intensity\n",
        "    Ir = Ir_x + Ir_y + Ir_z  # Total reactive intensity\n",
        "\n",
        "    return Ia, Ir\n"
      ],
      "metadata": {
        "id": "aMt7lTRd_lUw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AmbisonicDataset(Dataset):\n",
        "    def __init__(self, data, config):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (list of dicts): Each dictionary contains paths and labels for a sample.\n",
        "            config (dict): Configuration dictionary including sample rate (sr), duration, etc.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.config = config\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Load and process ambisonic audio\n",
        "        speech_path = sample['speech_path']\n",
        "        noise_path = sample['noise_path'] if 'noise_path' in sample and sample['noise_path'] is not None else None\n",
        "\n",
        "        stft_audio = load_mix_and_stft_foa_audio(\n",
        "            speech_path,\n",
        "            noise_pth=noise_path,\n",
        "            sr=self.config['sr'],\n",
        "            duration=self.config['duration'],\n",
        "            noise_ratio=self.config['noise_ratio'],\n",
        "            n_fft=self.config['n_fft'],\n",
        "            hop_length=self.config['hop_length'],\n",
        "            win_length=self.config['win_length']\n",
        "        )\n",
        "\n",
        "        # Compute active and reactive intensities\n",
        "        Ia, Ir = compute_active_reactive_intensities(stft_audio, rho=1.21, c=343)\n",
        "\n",
        "        # Generate Spike Trains\n",
        "        spikes_Ia = spikegen.rate(Ia, num_steps=self.config[\"num_steps\"])\n",
        "        spikes_Ir = spikegen.rate(Ir, num_steps=self.config[\"num_steps\"])\n",
        "\n",
        "        azimuth = sample['azimuth']\n",
        "        elevation = sample['elevation']\n",
        "        label = torch.tensor([azimuth, elevation], dtype=torch.float)\n",
        "\n",
        "        return spikes_Ia, spikes_Ir, label\n"
      ],
      "metadata": {
        "id": "JNsmExR-IyMJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = filter_data()\n",
        "train_data, val_data, test_data = split_data(data)\n",
        "\n",
        "train_dataset = AmbisonicDataset(data=train_data, config=config)\n",
        "val_dataset = AmbisonicDataset(data=val_data, config=config)\n",
        "test_dataset = AmbisonicDataset(data=test_data, config=config)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "rJaX1it_aU4B"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for spikes_active, spikes_reactive, lables in train_loader:\n",
        "#     print(spikes_active.shape)\n",
        "#     print(spikes_reactive.shape)\n",
        "#     print(lables.shape)\n",
        "#     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uLnmiEuayje",
        "outputId": "b8a62000-c8b7-4c9e-9d62-3493f9d0ecc3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10, 257, 63])\n",
            "torch.Size([32, 10, 257, 63])\n",
            "torch.Size([32, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for spikes_Ia, spikes_Ir, _ in train_loader:\n",
        "#     spikes_per_step_Ia = spikes_Ia.sum(dim=[2, 3])\n",
        "#     spikes_per_step_Ir = spikes_Ir.sum(dim=[2, 3])\n",
        "#     print(f\"Spikes in Ia: {spikes_per_step_Ia}\")\n",
        "#     print(f\"Spikes in Ir: {spikes_per_step_Ir}\")\n",
        "#     break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EpXey0icmpI",
        "outputId": "c1cda523-0ac8-4281-cca2-bef28ba59bc8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spikes in Ia: tensor([[155., 159., 155., 147., 148., 144., 156., 135., 165., 151.],\n",
            "        [ 89.,  95.,  87.,  92.,  96., 100.,  85.,  90.,  78., 115.],\n",
            "        [ 44.,  48.,  51.,  54.,  50.,  44.,  46.,  53.,  50.,  47.],\n",
            "        [114., 109., 135., 126., 112., 131., 130., 125., 135., 118.],\n",
            "        [115., 122., 134., 131., 129., 140., 123., 140., 130., 136.],\n",
            "        [123., 116., 140., 136., 122., 117., 131., 120., 129., 139.],\n",
            "        [ 75.,  58.,  80.,  71.,  77.,  57.,  71.,  65.,  68.,  87.],\n",
            "        [109., 115., 100., 109., 110., 114.,  88., 112.,  96., 117.],\n",
            "        [ 10.,  17.,  13.,  14.,  14.,  12.,  10.,  12.,  15.,  11.],\n",
            "        [150., 179., 162., 148., 172., 172., 146., 161., 161., 154.],\n",
            "        [109., 133., 130., 112., 111., 109., 101., 114., 114., 123.],\n",
            "        [ 52.,  51.,  49.,  47.,  59.,  53.,  52.,  52.,  51.,  58.],\n",
            "        [ 10.,   9.,   7.,  11.,   8.,  12.,  11.,  10.,   7.,   8.],\n",
            "        [100.,  95.,  99., 101.,  98.,  92.,  90.,  99., 101.,  98.],\n",
            "        [342., 331., 360., 346., 347., 347., 345., 331., 353., 347.],\n",
            "        [133., 130., 133., 140., 155., 145., 110., 129., 133., 127.],\n",
            "        [134., 109., 114., 127., 107., 111., 120., 133., 118., 117.],\n",
            "        [ 65.,  66.,  58.,  50.,  59.,  51.,  58.,  45.,  55.,  58.],\n",
            "        [ 76.,  75.,  69.,  85.,  64.,  74.,  74.,  83.,  73.,  73.],\n",
            "        [110., 121., 111., 129., 109., 127., 125., 107., 121., 109.],\n",
            "        [ 66., 106., 105., 108., 119., 100., 116.,  98., 116.,  98.],\n",
            "        [ 89.,  94.,  87.,  97.,  86.,  87.,  93., 108., 109.,  98.],\n",
            "        [ 26.,  35.,  25.,  31.,  25.,  27.,  35.,  24.,  29.,  34.],\n",
            "        [114., 110., 113., 113., 122.,  99., 116., 117., 117., 117.],\n",
            "        [ 50.,  52.,  47.,  45.,  46.,  41.,  41.,  45.,  46.,  49.],\n",
            "        [247., 199., 250., 226., 237., 249., 230., 246., 231., 211.],\n",
            "        [ 46.,  40.,  49.,  48.,  41.,  47.,  41.,  58.,  48.,  51.],\n",
            "        [150., 138., 136., 133., 129., 128., 129., 120., 141., 136.],\n",
            "        [ 67.,  69.,  56.,  62.,  59.,  59.,  52.,  60.,  75.,  63.],\n",
            "        [ 16.,  18.,  22.,  25.,  24.,  25.,  23.,  20.,  22.,  25.],\n",
            "        [127., 119., 126., 120., 115., 128., 120., 121., 119., 128.],\n",
            "        [ 27.,  27.,  26.,  27.,  27.,  31.,  27.,  32.,  28.,  34.]])\n",
            "Spikes in Ir: tensor([[ 67.,  69.,  63.,  52.,  63.,  54.,  59.,  65.,  67.,  56.],\n",
            "        [ 50.,  50.,  53.,  50.,  49.,  54.,  54.,  44.,  56.,  53.],\n",
            "        [ 45.,  36.,  42.,  37.,  26.,  31.,  28.,  57.,  36.,  32.],\n",
            "        [ 18.,  33.,  20.,  36.,  24.,  24.,  26.,  25.,  30.,  27.],\n",
            "        [ 34.,  37.,  29.,  36.,  33.,  36.,  31.,  34.,  42.,  39.],\n",
            "        [ 18.,  22.,  15.,  19.,  22.,  16.,  20.,  14.,  31.,  20.],\n",
            "        [ 51.,  46.,  51.,  44.,  47.,  46.,  46.,  44.,  45.,  52.],\n",
            "        [ 34.,  44.,  42.,  36.,  42.,  44.,  33.,  38.,  38.,  44.],\n",
            "        [ 54.,  57.,  45.,  60.,  68.,  63.,  59.,  58.,  67.,  65.],\n",
            "        [ 75.,  80.,  91.,  76.,  93.,  76.,  90.,  96.,  86.,  72.],\n",
            "        [ 55.,  71.,  61.,  70.,  53.,  54.,  64.,  67.,  55.,  70.],\n",
            "        [ 28.,  22.,  24.,  26.,  22.,  21.,  23.,  26.,  25.,  27.],\n",
            "        [ 29.,  22.,  28.,  29.,  26.,  24.,  26.,  28.,  19.,  35.],\n",
            "        [ 19.,  10.,  15.,  20.,  17.,  20.,  27.,  19.,  29.,  18.],\n",
            "        [123., 119., 136., 120., 123., 110., 118., 106., 121., 108.],\n",
            "        [129., 126., 126., 127., 142., 132., 121., 122., 120., 135.],\n",
            "        [ 31.,  33.,  30.,  28.,  33.,  28.,  31.,  26.,  26.,  27.],\n",
            "        [ 64.,  56.,  64.,  63.,  69.,  82.,  74.,  60.,  75.,  66.],\n",
            "        [ 49.,  50.,  58.,  45.,  47.,  43.,  47.,  58.,  60.,  49.],\n",
            "        [ 35.,  36.,  37.,  40.,  37.,  42.,  28.,  27.,  43.,  36.],\n",
            "        [ 63.,  62.,  58.,  54.,  42.,  56.,  50.,  49.,  53.,  56.],\n",
            "        [ 69.,  70.,  73.,  68.,  56.,  63.,  67.,  61.,  66.,  66.],\n",
            "        [ 27.,  24.,  24.,  32.,  24.,  23.,  31.,  38.,  25.,  32.],\n",
            "        [ 25.,  23.,  39.,  38.,  33.,  29.,  37.,  27.,  34.,  30.],\n",
            "        [ 52.,  52.,  49.,  50.,  53.,  50.,  59.,  48.,  52.,  46.],\n",
            "        [ 64.,  75.,  88.,  57.,  71.,  67.,  80.,  62.,  67.,  70.],\n",
            "        [ 70.,  50.,  56.,  60.,  55.,  55.,  53.,  63.,  62.,  56.],\n",
            "        [107., 110., 114., 107., 111., 112., 112., 100., 129., 110.],\n",
            "        [ 59.,  50.,  51.,  55.,  58.,  54.,  53.,  67.,  59.,  62.],\n",
            "        [ 39.,  47.,  33.,  43.,  41.,  39.,  51.,  45.,  58.,  37.],\n",
            "        [ 50.,  60.,  43.,  49.,  54.,  57.,  56.,  50.,  47.,  54.],\n",
            "        [105.,  80.,  99.,  94.,  84., 100.,  90.,  98., 102.,  92.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(SNN, self).__init__()\n",
        "        self.num_steps\n",
        "        self.beta = config[\"beta\"]\n",
        "\n",
        "        # snntorch LIF neuron\n",
        "        self.lif_neuron = snn.Leaky(beta=self.beta,)\n",
        "\n",
        "        # Define the active branch with LIF neurons\n",
        "        self.active_branch = nn.Sequential(\n",
        "            nn.Conv2d(1, 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(2),\n",
        "            nn.MaxPool2d(2),\n",
        "            # LIF neuron\n",
        "\n",
        "            nn.Conv2d(2, 4, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.MaxPool2d(2),\n",
        "            # LIF neuron\n",
        "\n",
        "            nn.Conv2d(4, 8, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.MaxPool2d(2),\n",
        "            # LIF neuron\n",
        "\n",
        "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.MaxPool2d(2)\n",
        "            # LIF neuron\n",
        "        )\n",
        "\n",
        "        self.reactive_branch = nn.Sequential(\n",
        "            nn.Conv2d(1, 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(2),\n",
        "            nn.MaxPool2d(2),\n",
        "            # LIF neuron\n",
        "        )\n",
        "\n",
        "        # MLP for concatenated features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(16*16*16, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, active_input, reactive_input):\n",
        "        # Apply convolutions and LIF neurons per timestep\n",
        "\n",
        "        active_output = self.active_branch(active_spikes)\n",
        "        reactive_output = self.reactive_branch(reactive_spikes)\n",
        "\n",
        "        # Concatenate, flatten, and pass through MLP\n",
        "        combined = torch.cat((active_output, reactive_output), dim=1)\n",
        "        combined = combined.view(combined.size(0), -1)  # Flatten\n",
        "        output = self.fc(combined)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "GNnMVrkNlU-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jjquDUYFhDmf"
      },
      "outputs": [],
      "source": [
        "# def calc_median_absolute_error(\n",
        "#         t_azimuth,\n",
        "#         t_elevation,\n",
        "#         p_azimuth,\n",
        "#         p_elevation\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     From Section 4: https://www.isca-archive.org/interspeech_2023/sarabia23_interspeech.html\n",
        "\n",
        "#     Parameters:\n",
        "#     true_azimuth (float): Azimuth angle of the true point in radians.\n",
        "#     true_elevation (float): Elevation angle of the true point in radians.\n",
        "#     pred_azimuth (float): Azimuth angle of the predicted point in radians.\n",
        "#     pred_elevation (float): Elevation angle of the predicted point in radians.\n",
        "\n",
        "#     Returns:\n",
        "#     float: The angular distance in degrees.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Calc angle error in radians\n",
        "#     error_rad = torch.acos(\n",
        "#         torch.sin(true_azimuth) * torch.sin(pred_azimuth) +\n",
        "#         torch.cos(true_azimuth) * torch.cos(pred_azimuth) * torch.cos(true_elevation - pred_elevation)\n",
        "#     )\n",
        "\n",
        "#     # Convert radians to degrees\n",
        "#     error_deg = torch.rad2deg(error_rad)\n",
        "\n",
        "#     # Calc median\n",
        "#     median_error = torch.median(torch.abs(error_deg))\n",
        "\n",
        "#     return median_error\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform and normalize segments of spatial audio encoded as 4-channel first-order ambisonics into active and reactive compents\n",
        "    # The duration of the segments varies by task 3D source localization 0.5s\n",
        "\n",
        "# Active and reactive compenets are fed through two independent branches of four 3D-convolutional layers each of 2,4,8,16 channels respectively\n",
        "    # with max-pool batch-norm and exponential linear units between the convolution layers.\n",
        "    # The output of both branches is flattened and concatenated and fed to a 3-layer multilayer perception\n",
        "\n",
        "# Trained for 20 Epochs\n"
      ],
      "metadata": {
        "id": "ILcXWWL1kRcB"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}