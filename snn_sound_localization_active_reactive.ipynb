{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ogIzFacrhYZD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "from snntorch import utils\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import optuna\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nXnOrdx29edp"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # Filter Data\n",
        "    \"metadata_path\": \"/home/emercad3/sound_localization/data/metadata.parquet\",\n",
        "    \"speech_path\": \"/home/emercad3/sound_localization/data/ambisonics\",\n",
        "    \"noise_path\": \"/home/emercad3/sound_localization/data/noise_ambisonics\",\n",
        "    \"is_lite_version\": True,\n",
        "\n",
        "    # Load Mix and STFT Audio\n",
        "    \"sr\": 16_000,\n",
        "    \"duration\": 0.5,\n",
        "    \"noise_ratio\": None, # Optional (Large #s make noise louder, Small #s make noise quieter)\n",
        "    \"n_fft\": 512,\n",
        "    \"hop_length\": 128,\n",
        "    \"win_length\": 512,\n",
        "\n",
        "    # Split Data\n",
        "    \"val_size\": 0.1,\n",
        "\n",
        "    # Dataset\n",
        "    \"num_steps\": 10,\n",
        "    \"batch_size\": 128,\n",
        "\n",
        "    # SNN\n",
        "    \"do\": 0.6,\n",
        "    \"beta\": 0.65,\n",
        "    \"beta_re\": 0.65,\n",
        "    \"beta_out\": 0.65,\n",
        "        # Active Branch\n",
        "    \"a_thresh1\": 10, \"a_thresh2\": 10, \"a_thresh3\": 10, \"a_thresh4\": 10,\n",
        "        # Reactive Branch\n",
        "    \"re_thresh1\": 10, \"re_thresh2\": 10, \"re_thresh3\": 10, \"re_thresh4\": 10,\n",
        "        # Output Layer\n",
        "    \"out_thresh1\": 10, \"out_thresh2\": 10,\n",
        "\n",
        "    # Training\n",
        "    \"num_epochs\": 80,\n",
        "    \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    \"lr\": 0.0003,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T-yb90H8ry_W"
      },
      "outputs": [],
      "source": [
        "def filter_data(\n",
        "        metadata_pth=config[\"metadata_path\"],\n",
        "        speech_pth=config[\"speech_path\"],\n",
        "        noise_pth=config[\"noise_path\"],\n",
        "        lite_version=config[\"is_lite_version\"]\n",
        "):\n",
        "\n",
        "    metadata = pd.read_parquet(metadata_pth, engine=\"pyarrow\")\n",
        "    if lite_version:\n",
        "        metadata = metadata[metadata[\"lite_version\"] == True]\n",
        "\n",
        "    data = []\n",
        "    for _, row in metadata.iterrows():\n",
        "        sample = {\n",
        "            \"sample_id\": row[\"sample_id\"],\n",
        "            \"speech_path\": os.path.join(speech_pth, f\"{row['sample_id']:06}.flac\"),\n",
        "            \"noise_path\": os.path.join(noise_pth, f\"{row['sample_id']:06}.flac\") if noise_pth else None,\n",
        "            \"azimuth\": row[\"speech/azimuth\"],\n",
        "            \"elevation\": row[\"speech/elevation\"],\n",
        "            \"split\": row[\"split\"]\n",
        "        }\n",
        "        data.append(sample)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KzoFwpF4WMFP"
      },
      "outputs": [],
      "source": [
        "def split_data(data, val_size=config[\"val_size\"]):\n",
        "    train_data = [sample for sample in data if sample[\"split\"] == \"train\"]\n",
        "    test_data = [sample for sample in data if sample[\"split\"] == \"test\"]\n",
        "\n",
        "    train_data, val_data = train_test_split(train_data, test_size=val_size, random_state=42)\n",
        "\n",
        "    return train_data, val_data, test_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load, Trim/Pad, STFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gKUl7kz-uRUU"
      },
      "outputs": [],
      "source": [
        "def load_mix_and_stft_foa_audio(\n",
        "        speech_pth,\n",
        "        noise_pth=None,\n",
        "        sr=config[\"sr\"],\n",
        "        duration=config[\"duration\"],\n",
        "        noise_ratio=config[\"noise_ratio\"],\n",
        "        n_fft=config[\"n_fft\"],\n",
        "        hop_length=config[\"hop_length\"],\n",
        "        win_length=config[\"win_length\"],\n",
        "        device=config[\"device\"]\n",
        "):\n",
        "\n",
        "    def preprocess_audio(audio_pth, sr, target_len, device):\n",
        "        num_frames = target_len\n",
        "        audio, audio_sr = torchaudio.load(audio_pth, frame_offset=0, num_frames=num_frames)\n",
        "        audio = audio.to(device)  # Move to GPU\n",
        "        if audio_sr != sr:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=audio_sr, new_freq=sr)\n",
        "            audio = resample_transform(audio)\n",
        "        max_val = audio.abs().max()\n",
        "        if max_val > 0:  # Avoid division by zero\n",
        "            audio = audio / max_val\n",
        "        # Check if padding is necessary (after resampling, the actual number of samples might change)\n",
        "        actual_len = audio.size(1)\n",
        "        if actual_len < target_len:\n",
        "            padding_size = target_len - actual_len\n",
        "            audio = torch.nn.functional.pad(audio, (0, padding_size), \"constant\", 0)\n",
        "        return audio\n",
        "\n",
        "    device = device\n",
        "    target_len = int(duration * sr)\n",
        "    speech_audio = preprocess_audio(speech_pth, sr, target_len, device)\n",
        "    should_renormalize = False\n",
        "\n",
        "    if noise_pth is not None:\n",
        "        noise_audio = preprocess_audio(noise_pth, sr, target_len, device)\n",
        "\n",
        "        if noise_ratio is not None:\n",
        "            # Adjust noise level relative to speech\n",
        "            noise_audio = noise_audio * noise_ratio\n",
        "            should_renormalize = True\n",
        "\n",
        "        # Mix speech and noise\n",
        "        mixed_audio = speech_audio + noise_audio\n",
        "    else:\n",
        "        mixed_audio = speech_audio\n",
        "\n",
        "    if should_renormalize:\n",
        "        # Re-normalize only if noise has been adjusted and mixed\n",
        "        max_val = mixed_audio.abs().max()\n",
        "        if max_val > 0:\n",
        "            mixed_audio = mixed_audio / max_val\n",
        "\n",
        "    # Move Window to device\n",
        "    window = torch.hann_window(win_length).to(device)\n",
        "\n",
        "    # Compute the STFT of the mixed audio\n",
        "    stft = torch.stft(mixed_audio,\n",
        "                      n_fft=n_fft,\n",
        "                      hop_length=hop_length,\n",
        "                      win_length=win_length,\n",
        "                      window=window,\n",
        "                      center=True,\n",
        "                      normalized=True,\n",
        "                      onesided=False,\n",
        "                      return_complex=True)\n",
        "\n",
        "    return stft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate Active and Reactive Intensity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aMt7lTRd_lUw"
      },
      "outputs": [],
      "source": [
        "def compute_active_reactive_intensities(stft, rho=1.21, c=343):\n",
        "    \"\"\"\n",
        "    Compute active and reactive intensity vectors from STFT of 4-channel FOA audio.\n",
        "    Args:\n",
        "    - stft: STFT of the FOA audio with shape [4, Frequency Bins, Time Frames].\n",
        "    - rho: Mean density of air (in kg/m^3).\n",
        "    - c: Speed of sound in air (in m/s).\n",
        "\n",
        "    Returns:\n",
        "    - Ia: Active intensity vector.\n",
        "    - Ir: Reactive intensity vector.\n",
        "    \"\"\"\n",
        "    # Constants\n",
        "    three = torch.tensor(3.0, dtype=torch.float, device=stft.device)\n",
        "    normalization_factor = -1 / (rho * c * torch.sqrt(three))\n",
        "\n",
        "    # Extract channels\n",
        "    p = stft[0]  # Pressure (W channel)\n",
        "    vx = stft[1] * normalization_factor  # Velocity X\n",
        "    vy = stft[2] * normalization_factor  # Velocity Y\n",
        "    vz = stft[3] * normalization_factor  # Velocity Z\n",
        "\n",
        "    # Compute complex conjugate of pressure\n",
        "    p_star = torch.conj(p)\n",
        "\n",
        "    # Calculate active and reactive intensity vectors\n",
        "    Ia_x = torch.real(p_star * vx)\n",
        "    Ia_y = torch.real(p_star * vy)\n",
        "    Ia_z = torch.real(p_star * vz)\n",
        "\n",
        "    Ir_x = torch.imag(p_star * vx)\n",
        "    Ir_y = torch.imag(p_star * vy)\n",
        "    Ir_z = torch.imag(p_star * vz)\n",
        "\n",
        "    # Create stack for each channel [3, num_samples, num_frames]\n",
        "    Ia = torch.stack((Ia_x, Ia_y, Ia_z), dim=0)\n",
        "    Ir = torch.stack((Ir_x, Ir_y, Ir_z), dim=0)\n",
        "\n",
        "    return Ia, Ir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JNsmExR-IyMJ"
      },
      "outputs": [],
      "source": [
        "class AmbisonicDataset(Dataset):\n",
        "    def __init__(self, data, config):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (list of dicts): Each dictionary contains paths and labels for a sample.\n",
        "            config (dict): Configuration dictionary including sample rate (sr), duration, etc.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.config = config\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Load and process ambisonic audio\n",
        "        speech_path = sample[\"speech_path\"]\n",
        "        noise_path = sample[\"noise_path\"] if \"noise_path\" in sample and sample[\"noise_path\"] is not None else None\n",
        "\n",
        "        stft_audio = load_mix_and_stft_foa_audio(\n",
        "            speech_path,\n",
        "            noise_pth=noise_path,\n",
        "            sr=self.config[\"sr\"],\n",
        "            duration=self.config[\"duration\"],\n",
        "            noise_ratio=self.config[\"noise_ratio\"],\n",
        "            n_fft=self.config[\"n_fft\"],\n",
        "            hop_length=self.config[\"hop_length\"],\n",
        "            win_length=self.config[\"win_length\"],\n",
        "            device=self.config[\"device\"]\n",
        "        )\n",
        "\n",
        "        # Compute active and reactive intensities\n",
        "        Ia, Ir = compute_active_reactive_intensities(stft_audio, rho=1.21, c=343)\n",
        "\n",
        "        # Generate Spike Trains\n",
        "        spikes_Ia = spikegen.rate(Ia, num_steps=self.config[\"num_steps\"])\n",
        "        spikes_Ir = spikegen.rate(Ir, num_steps=self.config[\"num_steps\"])\n",
        "\n",
        "        azimuth = sample['azimuth']\n",
        "        elevation = sample['elevation']\n",
        "        label = torch.tensor([azimuth, elevation], dtype=torch.float)\n",
        "\n",
        "        #print(f\"stft_audio.shape: {stft_audio.shape}\\nIa.shape:{Ia.shape}, Ir.shape: {Ir.shape}\\nspikes_Ia.shape: {spikes_Ia.shape}, spikes_Ir.shape: {spikes_Ir.shape}\\nlabel.shape: {label.shape}\")\n",
        "\n",
        "        return spikes_Ia, spikes_Ir, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GNnMVrkNlU-6"
      },
      "outputs": [],
      "source": [
        "class SNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(SNN, self).__init__()\n",
        "        self.num_steps = config[\"num_steps\"]\n",
        "        self.beta = config[\"beta\"]\n",
        "        self.beta_re = config[\"beta_re\"]\n",
        "        self.beta_out = config[\"beta_out\"]\n",
        "\n",
        "        # Active Thresholds\n",
        "        self.a_thr1 = config[\"a_thresh1\"]\n",
        "        self.a_thr2 = config[\"a_thresh2\"]\n",
        "        self.a_thr3 = config[\"a_thresh3\"]\n",
        "        self.a_thr4 = config[\"a_thresh4\"]\n",
        "\n",
        "        # Reactive Thresholds\n",
        "        self.re_thr1 = config[\"re_thresh1\"]\n",
        "        self.re_thr2 = config[\"re_thresh2\"]\n",
        "        self.re_thr3 = config[\"re_thresh3\"]\n",
        "        self.re_thr4 = config[\"re_thresh4\"]\n",
        "\n",
        "        ## FC Layer\n",
        "        self.beta_out = config[\"beta_out\"]\n",
        "        self.out_thr1 = config[\"out_thresh1\"]\n",
        "        self.out_thr2 = config[\"out_thresh2\"]\n",
        "        self.do = config[\"do\"]\n",
        "\n",
        "        # Define the active branch with LIF neurons\n",
        "        self.active_branch = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr1, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(6, 12, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr2, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(12, 24, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr3, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(24, 48, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr4, init_hidden=True),\n",
        "        )\n",
        "\n",
        "        self.reactive_branch = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta_re, threshold=self.re_thr1, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(6, 12, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta_re, threshold=self.re_thr2, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(12, 24, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta_re, threshold=self.re_thr3, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(24, 48, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta_re, threshold=self.re_thr4, init_hidden=True),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(9216, 512), # use 4608 when stft onesided=True\n",
        "            snn.Leaky(beta=self.beta_out, threshold=self.out_thr1, init_hidden=True),\n",
        "            nn.Linear(512, 256),\n",
        "            snn.Leaky(beta=self.beta_out, threshold=self.out_thr2, init_hidden=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Dropout(p=self.do),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    # def forward(self, active_input, reactive_input):\n",
        "    #     active_in_permuted = active_input.permute(1, 0, 2, 3, 4) # [num_step, batch_size, channels, num_samples, num_frames]\n",
        "    #     reactive_in_permuted = reactive_input.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "    #     active_outputs = []\n",
        "    #     reactive_outputs = []\n",
        "\n",
        "    #     for step in range(active_in_permuted.size(0)):\n",
        "    #         active_step = self.active_branch(active_in_permuted[step])\n",
        "    #         reactive_step = self.reactive_branch(reactive_in_permuted[step])\n",
        "\n",
        "    #         active_outputs.append(active_step)\n",
        "    #         reactive_outputs.append(reactive_step)\n",
        "\n",
        "    #     # Stack the outputs across time steps to form tensors of shape [num_steps, batch_size, channels, height, width]\n",
        "    #     active_stacked = torch.stack(active_outputs, dim=0)\n",
        "    #     reactive_stacked = torch.stack(reactive_outputs, dim=0)\n",
        "\n",
        "    #     # Aggregate across time steps, e.g., by taking the mean or sum\n",
        "    #     active_agg = torch.mean(active_stacked, dim=0)\n",
        "    #     reactive_agg = torch.mean(reactive_stacked, dim=0)\n",
        "\n",
        "    #     # Flatten and concatenate the aggregated outputs for the final MLP\n",
        "    #     active_flat = active_agg.view(active_agg.size(0), -1)\n",
        "    #     reactive_flat = reactive_agg.view(reactive_agg.size(0), -1)\n",
        "    #     combined = torch.cat((active_flat, reactive_flat), dim=1)\n",
        "\n",
        "    #     output = self.fc(combined)\n",
        "\n",
        "    #     return output\n",
        "\n",
        "    def forward(self, active_input, reactive_input):\n",
        "        step_outputs = []\n",
        "        permute_active = active_input.permute(1, 0, 2, 3, 4)\n",
        "        permute_reactive = reactive_input.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        if permute_active.size(0) != permute_reactive.size(0):\n",
        "            raise ValueError(\"The Tme Steps from active and reactive Do NOT Match\")\n",
        "\n",
        "        for step in range(permute_active.size(0)):\n",
        "\n",
        "            current_active = permute_active[step]\n",
        "            current_reactive = permute_reactive[step]\n",
        "\n",
        "            # Process inputs through active and reactive branches\n",
        "            active_out = self.active_branch(current_active)\n",
        "            reactive_out = self.reactive_branch(current_reactive)\n",
        "\n",
        "            # Flatten and combine the outputs\n",
        "            combined = torch.cat((active_out, reactive_out), dim=1)\n",
        "            combined = combined.view(combined.size(0), -1)\n",
        "\n",
        "            fc_out = self.fc(combined)\n",
        "\n",
        "            step_outputs.append(fc_out)\n",
        "\n",
        "        tensor_out = torch.stack(step_outputs, dim=0)\n",
        "        output = torch.mean(tensor_out, dim=0)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jjquDUYFhDmf"
      },
      "outputs": [],
      "source": [
        "def calc_median_absolute_error(t_azimuth, t_elevation, p_azimuth, p_elevation):\n",
        "    \"\"\"\n",
        "    Calculate the median absolute error of the angular distance between the true\n",
        "    and predicted azimuth and elevation angles.\n",
        "\n",
        "    Parameters:\n",
        "    t_azimuth (tensor): Azimuth angles of the true points in radians.\n",
        "    t_elevation (tensor): Elevation angles of the true points in radians.\n",
        "    p_azimuth (tensor): Azimuth angles of the predicted points in radians.\n",
        "    p_elevation (tensor): Elevation angles of the predicted points in radians.\n",
        "\n",
        "    Returns:\n",
        "    tensor: The median angular distance in degrees.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the cosine of the angular distance\n",
        "    cosine_of_angle = (\n",
        "        torch.sin(t_azimuth) * torch.sin(p_azimuth) +\n",
        "        torch.cos(t_azimuth) * torch.cos(p_azimuth) * torch.cos(t_elevation - p_elevation)\n",
        "    )\n",
        "\n",
        "    # Clamp the cosine of the angle to the range [-1, 1] to avoid errors due to numerical instability\n",
        "    cosine_of_angle = torch.clamp(cosine_of_angle, -1, 1)\n",
        "\n",
        "    # Calculate the angular distance in radians\n",
        "    error_rad = torch.acos(cosine_of_angle)\n",
        "\n",
        "    # Convert the angular distance from radians to degrees\n",
        "    error_deg = torch.rad2deg(error_rad)\n",
        "\n",
        "    # Calculate the median of the absolute errors in degrees\n",
        "    median_error = torch.median(torch.abs(error_deg))\n",
        "\n",
        "    return median_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training, Validation and Testing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-UHrSaeJgv_X"
      },
      "outputs": [],
      "source": [
        "# def train(model, train_loader, criterion, optimizer, device):\n",
        "#     model.train()  # Set model to training mode\n",
        "\n",
        "#     train_loss = 0.0\n",
        "#     true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "#     for batch_idx, (active_input, reactive_input, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "#         #print(f\"Batch {batch_idx+1}/{len(train_loader)}\")\n",
        "\n",
        "#         # Move data and labels to the device\n",
        "#         active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "#         # Reset branches - required for init_hidden=True\n",
        "#         utils.reset(model.active_branch)\n",
        "#         utils.reset(model.reactive_branch)\n",
        "\n",
        "#         # Zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model(active_input, reactive_input)  # Pass both inputs to the model\n",
        "\n",
        "#         # Compute loss\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # Backward pass and optimize\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Update training loss\n",
        "#         train_loss = train_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "#         true_azimuths.append(labels[:, 0].detach())\n",
        "#         true_elevations.append(labels[:, 1].detach())\n",
        "#         pred_azimuths.append(outputs[:, 0].detach())\n",
        "#         pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "#     # Calculate average loss over the dataset\n",
        "#     train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "#     return train_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    train_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    for batch_idx, (active_input, reactive_input, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        # Move data and labels to the device\n",
        "        active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "        # Check if model is wrapped in DataParallel and access the original model for reset\n",
        "        if isinstance(model, nn.DataParallel):\n",
        "            utils.reset(model.module.active_branch)\n",
        "            utils.reset(model.module.reactive_branch)\n",
        "        else:\n",
        "            utils.reset(model.active_branch)\n",
        "            utils.reset(model.reactive_branch)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(active_input, reactive_input)  # Pass both inputs to the model\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update training loss\n",
        "        train_loss += loss.item() * active_input.size(0)\n",
        "\n",
        "        true_azimuths.append(labels[:, 0].detach())\n",
        "        true_elevations.append(labels[:, 1].detach())\n",
        "        pred_azimuths.append(outputs[:, 0].detach())\n",
        "        pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate average loss over the dataset\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    return train_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a3KVmEoEgyc8"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed\n",
        "        for batch_idx, (active_input, reactive_input, labels) in enumerate(tqdm(val_loader, desc=\"Validation\")):\n",
        "            #print(f\"Batch {batch_idx+1}/{len(val_loader)}\")\n",
        "\n",
        "            # Move the inputs and labels to the specified device\n",
        "            active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            outputs = model(active_input, reactive_input)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss = valid_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "            true_azimuths.append(labels[:, 0].detach())\n",
        "            true_elevations.append(labels[:, 1].detach())\n",
        "            pred_azimuths.append(outputs[:, 0].detach())\n",
        "            pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate the average loss over the dataset\n",
        "    valid_loss = valid_loss / len(val_loader.dataset)\n",
        "\n",
        "    return valid_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4xUJ05NBg27K"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    test_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed during testing\n",
        "        for batch_idx, (active_input, reactive_input, labels) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
        "            active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            outputs = model(active_input, reactive_input)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss = test_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "            # Optionally, accumulate metrics here\n",
        "            true_azimuths.append(labels[:, 0].detach())\n",
        "            true_elevations.append(labels[:, 1].detach())\n",
        "            pred_azimuths.append(outputs[:, 0].detach())\n",
        "            pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate the average loss over the dataset\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoaders, Training setup, Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1j_wWBIwg96V"
      },
      "outputs": [],
      "source": [
        "# data = filter_data()\n",
        "# train_data, val_data, test_data = split_data(data)\n",
        "\n",
        "# train_dataset = AmbisonicDataset(data=train_data, config=config)\n",
        "# val_dataset = AmbisonicDataset(data=val_data, config=config)\n",
        "# test_dataset = AmbisonicDataset(data=test_data, config=config)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tvCPyyKuhBhi"
      },
      "outputs": [],
      "source": [
        "# device = config[\"device\"]\n",
        "\n",
        "# model = SNN(config)\n",
        "# if torch.cuda.device_count() > 1:\n",
        "#     print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
        "#     model = torch.nn.DataParallel(model)\n",
        "# model.to(device)\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KNpbk7KRh_BP"
      },
      "outputs": [],
      "source": [
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# train_loss = []\n",
        "# valid_loss = []\n",
        "\n",
        "# for epoch in range(config[\"num_epochs\"]):\n",
        "#     print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "\n",
        "#     train_epoch_loss, train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation = train(model, train_loader, criterion, optimizer, device)\n",
        "#     valid_epoch_loss, valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation = validate(model, val_loader, criterion, device)\n",
        "\n",
        "#     train_loss.append(train_epoch_loss)\n",
        "#     valid_loss.append(valid_epoch_loss)\n",
        "\n",
        "#     train_angle_error = calc_median_absolute_error(train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation)\n",
        "#     valid_angle_error = calc_median_absolute_error(valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation)\n",
        "\n",
        "#     print(f\"Train Loss: {train_epoch_loss:.4f} | Validation Loss: {valid_epoch_loss:.4f}\")\n",
        "#     print(f\"Train Angle Error: {train_angle_error:.4f}° | Validation Angle Error: {valid_angle_error:.4f}°\\n\")\n",
        "\n",
        "# test_loss, test_true_azimuth, test_true_elevation, test_pred_azimuth, test_pred_elevation = test(model, test_loader, criterion, device)\n",
        "# test_angle_error = calc_median_absolute_error(test_true_azimuth, test_true_elevation, test_pred_azimuth, test_pred_elevation)\n",
        "\n",
        "# print(f\"Test Loss: {test_loss:.4f}\")\n",
        "# print(f\"Test Angle Error: {test_angle_error:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypyerparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eUbG7PSIQBSV"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Hyperparameters to optimize\n",
        "    config[\"a_thresh1\"] = trial.suggest_float(\"a_thresh1\", 2, 5)\n",
        "    config[\"a_thresh2\"] = trial.suggest_float(\"a_thresh2\", 7, 10)\n",
        "    config[\"a_thresh3\"] = trial.suggest_float(\"a_thresh3\", 11, 14)\n",
        "    config[\"a_thresh4\"] = trial.suggest_float(\"a_thresh4\", 15, 19)\n",
        "    config[\"beta\"] = trial.suggest_float(\"beta\", 0.1, 0.4)\n",
        "\n",
        "\n",
        "    config[\"re_thresh1\"] = trial.suggest_float(\"re_thresh1\", 19, 23)\n",
        "    config[\"re_thresh2\"] = trial.suggest_float(\"re_thresh2\", 17, 21)\n",
        "    config[\"re_thresh3\"] = trial.suggest_float(\"re_thresh3\", 25, 30)\n",
        "    config[\"re_thresh4\"] = trial.suggest_float(\"re_thresh4\", 17, 21)\n",
        "    config[\"beta_re\"] = trial.suggest_float(\"beta_re\", 0.5, 0.7)\n",
        "\n",
        "    config[\"out_thresh1\"] = trial.suggest_float(\"out_thresh1\", 2, 6)\n",
        "    config[\"out_thresh2\"] = trial.suggest_float(\"out_thresh2\", 7, 11)\n",
        "    config[\"beta_out\"] = trial.suggest_float(\"beta_out\", 0.4, 0.6)\n",
        "\n",
        "    config[\"do\"] = trial.suggest_float(\"do\", 0.6, 0.9)\n",
        "    config[\"num_steps\"] = trial.suggest_int(\"num_steps\", 15, 25)\n",
        "    config[\"lr\"] = trial.suggest_float(\"lr\", 1e-8, 1e-4)\n",
        "\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SNN(config)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.cuda.device_count()} GPUs.\")\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    data = filter_data()\n",
        "    train_data, _, _ = split_data(data)\n",
        "\n",
        "    train_dataset = AmbisonicDataset(data=train_data, config=config)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        train_loss, train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation = train(model, train_loader, criterion, optimizer, device)\n",
        "        train_angle_error = calc_median_absolute_error(train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation)\n",
        "        \n",
        "\n",
        "    return train_angle_error.item()  # Optuna minimizes this value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Study Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZT8BYIRQQODD"
      },
      "outputs": [],
      "source": [
        "def run_study():\n",
        "    study_name = \"snn_study\"\n",
        "    storage_name = \"sqlite:///{}.db\".format(study_name)\n",
        "\n",
        "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=5)\n",
        "\n",
        "    # Save the study to disk\n",
        "    study.trials_dataframe().to_csv(\"study_results.csv\")\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"    Value: \", trial.value)\n",
        "    print(\"    Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"      {key}: {value}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aTJaM96QRGA",
        "outputId": "2be71d01-e0a6-438c-befa-fbc322a7e1e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-03-02 16:58:58,491] A new study created in RDB with name: snn_study\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 2 GPUs.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [18:34<00:00,  9.28s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:58<00:00,  1.49s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:50<00:00,  1.42s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:50<00:00,  1.42s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:50<00:00,  1.42s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:50<00:00,  1.42s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:49<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.41s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.41s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:48<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.40s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:46<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:47<00:00,  1.39s/it]\n",
            "Training: 100%|█████████████████████████████████████████████████████████████████████| 120/120 [02:50<00:00,  1.42s/it]\n",
            "[I 2024-03-02 20:59:02,630] Trial 0 finished with value: 89.89599609375 and parameters: {'a_thresh1': 3.8337279670699447, 'a_thresh2': 8.49422397058603, 'a_thresh3': 13.908491662155697, 'a_thresh4': 18.3015666849694, 'beta': 0.1446471669465707, 're_thresh1': 20.899529371571795, 're_thresh2': 20.2637494299751, 're_thresh3': 29.989097426584802, 're_thresh4': 17.778760913612977, 'beta_re': 0.6943878128050377, 'out_thresh1': 5.351293256573192, 'out_thresh2': 8.238336315402393, 'beta_out': 0.4974802095656458, 'do': 0.8191901142816258, 'num_steps': 18, 'lr': 9.188173989511328e-06}. Best is trial 0 with value: 89.89599609375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 2 GPUs.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|                                                                               | 0/120 [00:01<?, ?it/s]\n",
            "[W 2024-03-02 20:59:07,424] Trial 1 failed with parameters: {'a_thresh1': 2.5862351879508094, 'a_thresh2': 8.161197794079532, 'a_thresh3': 11.89868914601514, 'a_thresh4': 16.48063583803242, 'beta': 0.33801130816296476, 're_thresh1': 22.17770404139466, 're_thresh2': 20.164367070367526, 're_thresh3': 29.20639576206206, 're_thresh4': 19.577342769716026, 'beta_re': 0.6820338230991225, 'out_thresh1': 4.248104104266464, 'out_thresh2': 10.532935603459288, 'beta_out': 0.5989654961253971, 'do': 0.6211928026739928, 'num_steps': 25, 'lr': 7.684353122796153e-05} because of the following error: OutOfMemoryError('Caught OutOfMemoryError in replica 0 on device 0.\\nOriginal Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\\n    output = module(*input, **kwargs)\\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/var/tmp/ipykernel_11008/3066093645.py\", line 127, in forward\\n    active_out = self.active_branch(current_active)\\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\\n    input = module(input)\\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\\n    return F.batch_norm(\\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\\n    return torch.batch_norm(\\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.58 GiB total capacity; 13.44 GiB already allocated; 33.31 MiB free; 13.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n').\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/var/tmp/ipykernel_11008/3611388682.py\", line 44, in objective\n",
            "    train_loss, train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation = train(model, train_loader, criterion, optimizer, device)\n",
            "  File \"/var/tmp/ipykernel_11008/2625495943.py\", line 23, in train\n",
            "    outputs = model(active_input, reactive_input)  # Pass both inputs to the model\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 171, in forward\n",
            "    outputs = self.parallel_apply(replicas, inputs, kwargs)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 181, in parallel_apply\n",
            "    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 89, in parallel_apply\n",
            "    output.reraise()\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 644, in reraise\n",
            "    raise exception\n",
            "torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n",
            "    output = module(*input, **kwargs)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/var/tmp/ipykernel_11008/3066093645.py\", line 127, in forward\n",
            "    active_out = self.active_branch(current_active)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.58 GiB total capacity; 13.44 GiB already allocated; 33.31 MiB free; 13.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\n",
            "[W 2024-03-02 20:59:07,425] Trial 1 failed with value None.\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/var/tmp/ipykernel_11008/3066093645.py\", line 127, in forward\n    active_out = self.active_branch(current_active)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n    return F.batch_norm(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.58 GiB total capacity; 13.44 GiB already allocated; 33.31 MiB free; 13.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mrun_study\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m storage_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.db\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study_name)\n\u001b[1;32m      5\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39mstudy_name, storage\u001b[38;5;241m=\u001b[39mstorage_name, load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save the study to disk\u001b[39;00m\n\u001b[1;32m      9\u001b[0m study\u001b[38;5;241m.\u001b[39mtrials_dataframe()\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudy_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[0;32mIn[17], line 44\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     41\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m---> 44\u001b[0m     train_loss, train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     train_angle_error \u001b[38;5;241m=\u001b[39m calc_median_absolute_error(train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_angle_error\u001b[38;5;241m.\u001b[39mitem()\n",
            "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactive_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreactive_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass both inputs to the model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/var/tmp/ipykernel_11008/3066093645.py\", line 127, in forward\n    active_out = self.active_branch(current_active)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n    return F.batch_norm(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 14.58 GiB total capacity; 13.44 GiB already allocated; 33.31 MiB free; 13.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ],
      "source": [
        "run_study()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNWoncUlt9c3PDUylcH4G0q",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
