{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAMoUDBzjcqrpdOm3e6i7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mercadoerik1031/snn-sound-localization/blob/new_approach/snn_sound_localization_active_reactive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XWQW4Zj-Cwt",
        "outputId": "56ec12d1-0f70-4f6a-b81a-9f0325342988"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install snntorch --quiet"
      ],
      "metadata": {
        "id": "0b1hp-2BO2-W"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ogIzFacrhYZD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    # Filter Data\n",
        "    \"metadata_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/metadata.parquet\",\n",
        "    \"speech_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/ambisonics_lite\",\n",
        "    \"noise_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/noise_ambisonics_lite\",\n",
        "    \"is_lite_version\": True,\n",
        "\n",
        "    # Load Mix and STFT Audio\n",
        "    \"sr\": 16_000,\n",
        "    \"duration\": 0.5,\n",
        "    \"noise_ratio\": None, # Optional (Large #s make noise louder, Small #s make noise quieter)\n",
        "    \"n_fft\": 512,\n",
        "    \"hop_length\": 128,\n",
        "    \"win_length\": 512,\n",
        "\n",
        "    # Split Data\n",
        "    \"val_size\": 0.15,\n",
        "\n",
        "    # Dataset\n",
        "    \"num_steps\": 10,\n",
        "    \"batch_size\": 32,\n",
        "\n",
        "    # SNN\n",
        "    \"beta\": 0.65,\n",
        "        # Active Branch\n",
        "    \"a_thresh1\": 10,\n",
        "    \"a_thresh2\": 10,\n",
        "    \"a_thresh3\": 10,\n",
        "    \"a_thresh4\": 10,\n",
        "        # Reactive Branch\n",
        "    \"re_thresh1\": 10,\n",
        "    \"re_thresh2\": 10,\n",
        "    \"re_thresh3\": 10,\n",
        "    \"re_thresh4\": 10,\n",
        "\n",
        "    # Training\n",
        "    \"num_epochs\": 20,\n",
        "    \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    \"lr\": 0.0003,\n",
        "}"
      ],
      "metadata": {
        "id": "nXnOrdx29edp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data(\n",
        "        metadata_pth=config[\"metadata_path\"],\n",
        "        speech_pth=config[\"speech_path\"],\n",
        "        noise_pth=config[\"noise_path\"],\n",
        "        lite_version=config[\"is_lite_version\"]\n",
        "):\n",
        "\n",
        "    metadata = pd.read_parquet(metadata_pth, engine=\"pyarrow\")\n",
        "    if lite_version:\n",
        "        metadata = metadata[metadata[\"lite_version\"] == True]\n",
        "\n",
        "    data = []\n",
        "    for _, row in metadata.iterrows():\n",
        "        sample = {\n",
        "            \"sample_id\": row[\"sample_id\"],\n",
        "            \"speech_path\": os.path.join(speech_pth, f\"{row['sample_id']:06}.flac\"),\n",
        "            \"noise_path\": os.path.join(noise_pth, f\"{row['sample_id']:06}.flac\") if noise_pth else None,\n",
        "            \"azimuth\": row[\"speech/azimuth\"],\n",
        "            \"elevation\": row[\"speech/elevation\"],\n",
        "            \"split\": row[\"split\"]\n",
        "        }\n",
        "        data.append(sample)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "T-yb90H8ry_W"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, val_size=config[\"val_size\"]):\n",
        "    train_data = [sample for sample in data if sample[\"split\"] == \"train\"]\n",
        "    test_data = [sample for sample in data if sample[\"split\"] == \"test\"]\n",
        "\n",
        "    train_data, val_data = train_test_split(train_data, test_size=val_size, random_state=42)\n",
        "\n",
        "    return train_data, val_data, test_data\n"
      ],
      "metadata": {
        "id": "KzoFwpF4WMFP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mix_and_stft_foa_audio(\n",
        "        speech_pth,\n",
        "        noise_pth=None,\n",
        "        sr=config[\"sr\"],\n",
        "        duration=config[\"duration\"],\n",
        "        noise_ratio=config[\"noise_ratio\"],\n",
        "        n_fft=config[\"n_fft\"],\n",
        "        hop_length=config[\"hop_length\"],\n",
        "        win_length=config[\"win_length\"]\n",
        "):\n",
        "    # Function to load, resample, normalize, and trim/pad audio\n",
        "    def preprocess_audio(audio_pth, sr, target_len):\n",
        "        audio, audio_sr = torchaudio.load(audio_pth)\n",
        "        if audio_sr != sr:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=audio_sr, new_freq=sr)\n",
        "            audio = resample_transform(audio)\n",
        "        max_val = audio.abs().max()\n",
        "        if max_val > 0:  # Avoid division by zero\n",
        "            audio = audio / max_val\n",
        "        if audio.size(1) > target_len:\n",
        "            audio = audio[:, :target_len]\n",
        "        elif audio.size(1) < target_len:\n",
        "            padding_size = target_len - audio.size(1)\n",
        "            audio = torch.nn.functional.pad(audio, (0, padding_size), \"constant\", 0)\n",
        "        return audio\n",
        "\n",
        "    target_len = int(duration * sr)\n",
        "    speech_audio = preprocess_audio(speech_pth, sr, target_len)\n",
        "    should_renormalize = False\n",
        "\n",
        "    if noise_pth is not None:\n",
        "        noise_audio = preprocess_audio(noise_pth, sr, target_len)\n",
        "\n",
        "        if noise_ratio is not None:\n",
        "            # Adjust noise level relative to speech\n",
        "            noise_audio = noise_audio * noise_ratio\n",
        "            should_renormalize = True\n",
        "\n",
        "        # Mix speech and noise\n",
        "        mixed_audio = speech_audio + noise_audio\n",
        "    else:\n",
        "        mixed_audio = speech_audio\n",
        "\n",
        "    if should_renormalize:\n",
        "        # Re-normalize only if noise has been adjusted and mixed\n",
        "        max_val = mixed_audio.abs().max()\n",
        "        if max_val > 0:\n",
        "            mixed_audio = mixed_audio / max_val\n",
        "\n",
        "    # Compute the STFT of the mixed audio\n",
        "    stft = torch.stft(mixed_audio,\n",
        "                      n_fft=n_fft,\n",
        "                      hop_length=hop_length,\n",
        "                      win_length=win_length,\n",
        "                      window=torch.hann_window(win_length),\n",
        "                      center=True,\n",
        "                      normalized=False,\n",
        "                      onesided=True,\n",
        "                      return_complex=True)\n",
        "\n",
        "    return stft\n"
      ],
      "metadata": {
        "id": "gKUl7kz-uRUU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_active_reactive_intensities(stft, rho=1.21, c=343):\n",
        "    \"\"\"\n",
        "    Compute active and reactive intensity vectors from STFT of 4-channel FOA audio.\n",
        "    Args:\n",
        "    - stft: STFT of the FOA audio with shape [4, Frequency Bins, Time Frames].\n",
        "    - rho: Mean density of air (in kg/m^3).\n",
        "    - c: Speed of sound in air (in m/s).\n",
        "\n",
        "    Returns:\n",
        "    - Ia: Active intensity vector.\n",
        "    - Ir: Reactive intensity vector.\n",
        "    \"\"\"\n",
        "    # Constants\n",
        "    three = torch.tensor(3.0, dtype=torch.float, device=stft.device)\n",
        "    normalization_factor = -1 / (rho * c * torch.sqrt(three))\n",
        "\n",
        "    # Extract channels\n",
        "    p = stft[0]  # Pressure (W channel)\n",
        "    vx = stft[1] * normalization_factor  # Velocity X\n",
        "    vy = stft[2] * normalization_factor  # Velocity Y\n",
        "    vz = stft[3] * normalization_factor  # Velocity Z\n",
        "\n",
        "    # Compute complex conjugate of pressure\n",
        "    p_star = torch.conj(p)\n",
        "\n",
        "    # Calculate active and reactive intensity vectors\n",
        "    Ia_x = torch.real(p_star * vx)\n",
        "    Ia_y = torch.real(p_star * vy)\n",
        "    Ia_z = torch.real(p_star * vz)\n",
        "\n",
        "    Ir_x = torch.imag(p_star * vx)\n",
        "    Ir_y = torch.imag(p_star * vy)\n",
        "    Ir_z = torch.imag(p_star * vz)\n",
        "\n",
        "    # Summing up the components to get total active and reactive intensities\n",
        "    # Ia = Ia_x + Ia_y + Ia_z  # Total active intensity\n",
        "    # Ir = Ir_x + Ir_y + Ir_z  # Total reactive intensity\n",
        "\n",
        "    # Create stack for each channel [3, num_samples, num_frames]\n",
        "    Ia = torch.stack((Ia_x, Ia_y, Ia_z), dim=0)\n",
        "    Ir = torch.stack((Ir_x, Ir_y, Ir_z), dim=0)\n",
        "\n",
        "    return Ia, Ir\n"
      ],
      "metadata": {
        "id": "aMt7lTRd_lUw"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AmbisonicDataset(Dataset):\n",
        "    def __init__(self, data, config):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (list of dicts): Each dictionary contains paths and labels for a sample.\n",
        "            config (dict): Configuration dictionary including sample rate (sr), duration, etc.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.config = config\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Load and process ambisonic audio\n",
        "        speech_path = sample['speech_path']\n",
        "        noise_path = sample['noise_path'] if 'noise_path' in sample and sample['noise_path'] is not None else None\n",
        "\n",
        "        stft_audio = load_mix_and_stft_foa_audio(\n",
        "            speech_path,\n",
        "            noise_pth=noise_path,\n",
        "            sr=self.config['sr'],\n",
        "            duration=self.config['duration'],\n",
        "            noise_ratio=self.config['noise_ratio'],\n",
        "            n_fft=self.config['n_fft'],\n",
        "            hop_length=self.config['hop_length'],\n",
        "            win_length=self.config['win_length']\n",
        "        )\n",
        "\n",
        "        # Compute active and reactive intensities\n",
        "        Ia, Ir = compute_active_reactive_intensities(stft_audio, rho=1.21, c=343)\n",
        "\n",
        "        # Generate Spike Trains\n",
        "        spikes_Ia = spikegen.rate(Ia, num_steps=self.config[\"num_steps\"])\n",
        "        spikes_Ir = spikegen.rate(Ir, num_steps=self.config[\"num_steps\"])\n",
        "\n",
        "        azimuth = sample['azimuth']\n",
        "        elevation = sample['elevation']\n",
        "        label = torch.tensor([azimuth, elevation], dtype=torch.float)\n",
        "\n",
        "        #print(f\"stft_audio.shape: {stft_audio.shape}\\nIa.shape:{Ia.shape}, Ir.shape: {Ir.shape}\\nspikes_Ia.shape: {spikes_Ia.shape}, spikes_Ir.shape: {spikes_Ir.shape}\\nlabel.shape: {label.shape}\")\n",
        "\n",
        "        return spikes_Ia, spikes_Ir, label\n"
      ],
      "metadata": {
        "id": "JNsmExR-IyMJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for spikes_active, spikes_reactive, lables in train_loader:\n",
        "#     print(spikes_active.shape)\n",
        "#     print(spikes_reactive.shape)\n",
        "#     print(lables.shape)\n",
        "#     break"
      ],
      "metadata": {
        "id": "6uLnmiEuayje"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for spikes_Ia, spikes_Ir, _ in train_loader:\n",
        "#     spikes_per_step_Ia = spikes_Ia.sum(dim=[2, 3])\n",
        "#     spikes_per_step_Ir = spikes_Ir.sum(dim=[2, 3])\n",
        "#     print(f\"Spikes in Ia: {spikes_per_step_Ia}\")\n",
        "#     print(f\"Spikes in Ir: {spikes_per_step_Ir}\")\n",
        "#     break\n"
      ],
      "metadata": {
        "id": "0EpXey0icmpI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(SNN, self).__init__()\n",
        "        self.num_steps = config[\"num_steps\"]\n",
        "        self.beta = config[\"beta\"]\n",
        "\n",
        "        # Active Thresholds\n",
        "        self.a_thr1 = config[\"a_thresh1\"]\n",
        "        self.a_thr2 = config[\"a_thresh2\"]\n",
        "        self.a_thr3 = config[\"a_thresh3\"]\n",
        "        self.a_thr4 = config[\"a_thresh4\"]\n",
        "\n",
        "        # Reactive Thresholds\n",
        "        self.re_thr1 = config[\"re_thresh1\"]\n",
        "        self.re_thr2 = config[\"re_thresh2\"]\n",
        "        self.re_thr3 = config[\"re_thresh3\"]\n",
        "        self.re_thr4 = config[\"re_thresh4\"]\n",
        "\n",
        "        # Define the active branch with LIF neurons\n",
        "        self.active_branch = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr1, init_hidden=True),\n",
        "            #snn.Leaky(beta=self.beta, threshold=self.a_thr1),\n",
        "\n",
        "            nn.Conv2d(6, 12, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr2, init_hidden=True),\n",
        "            #snn.Leaky(beta=self.beta, threshold=self.a_thr2),\n",
        "\n",
        "            nn.Conv2d(12, 24, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr3, init_hidden=True),\n",
        "            #snn.Leaky(beta=self.beta, threshold=self.a_thr3),\n",
        "\n",
        "            nn.Conv2d(24, 48, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr4, init_hidden=True),\n",
        "            #snn.Leaky(beta=self.beta, threshold=self.a_thr4),\n",
        "        )\n",
        "\n",
        "        self.reactive_branch = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.re_thr1, init_hidden=True),\n",
        "            #snn.Leaky(beta=self.beta, threshold=self.re_thr1),\n",
        "\n",
        "            nn.Conv2d(6, 12, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.re_thr2, init_hidden=True),\n",
        "            #snn.Leaky(beta=self.beta, threshold=self.re_thr2),\n",
        "\n",
        "            nn.Conv2d(12, 24, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.re_thr3, init_hidden=True),\n",
        "            #snn.Leaky(beta=self.beta, threshold=self.re_thr3),\n",
        "\n",
        "            nn.Conv2d(24, 48, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.re_thr4, init_hidden=True),\n",
        "            #snn.Leaky(beta=self.beta, threshold=self.re_thr4),\n",
        "        )\n",
        "\n",
        "        # MLP for concatenated features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(4608, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    # def forward(self, active_input, reactive_input):\n",
        "    #     active_in_permuted = active_input.permute(1, 0, 2, 3, 4) # [num_step, batch_size, channels, num_samples, num_frames]\n",
        "    #     reactive_in_permuted = reactive_input.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "    #     active_outputs = []\n",
        "    #     reactive_outputs = []\n",
        "\n",
        "    #     for step in range(active_in_permuted.size(0)):\n",
        "    #         active_step = self.active_branch(active_in_permuted[step])\n",
        "    #         reactive_step = self.reactive_branch(reactive_in_permuted[step])\n",
        "\n",
        "    #         active_outputs.append(active_step)\n",
        "    #         reactive_outputs.append(reactive_step)\n",
        "\n",
        "    #     # Stack the outputs across time steps to form tensors of shape [num_steps, batch_size, channels, height, width]\n",
        "    #     active_stacked = torch.stack(active_outputs, dim=0)\n",
        "    #     reactive_stacked = torch.stack(reactive_outputs, dim=0)\n",
        "\n",
        "    #     # Aggregate across time steps, e.g., by taking the mean or sum\n",
        "    #     active_agg = torch.mean(active_stacked, dim=0)\n",
        "    #     reactive_agg = torch.mean(reactive_stacked, dim=0)\n",
        "\n",
        "    #     # Flatten and concatenate the aggregated outputs for the final MLP\n",
        "    #     active_flat = active_agg.view(active_agg.size(0), -1)\n",
        "    #     reactive_flat = reactive_agg.view(reactive_agg.size(0), -1)\n",
        "    #     combined = torch.cat((active_flat, reactive_flat), dim=1)\n",
        "\n",
        "    #     output = self.fc(combined)\n",
        "\n",
        "    #     return output\n",
        "\n",
        "    def forward(self, active_input, reactive_input):\n",
        "        active_in_permuted = active_input.permute(1, 0, 2, 3, 4)  # [num_step, batch_size, channels, height, width]\n",
        "        reactive_in_permuted = reactive_input.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        step_outputs = []\n",
        "\n",
        "        for step in range(active_in_permuted.size(0)):\n",
        "            active_step = self.active_branch(active_in_permuted[step])\n",
        "            reactive_step = self.reactive_branch(reactive_in_permuted[step])\n",
        "\n",
        "            active_step = active_step.view(active_step.size(0), -1) # Flatten\n",
        "            reactive_step = reactive_step.view(reactive_step.size(0), -1) # Flatten\n",
        "\n",
        "            combined = torch.cat((active_step, reactive_step), dim=1)\n",
        "\n",
        "            out = self.fc(combined)\n",
        "\n",
        "            step_outputs.append(out)\n",
        "\n",
        "        step_outputs_tensor = torch.stack(step_outputs, dim=0)\n",
        "        output = torch.mean(step_outputs_tensor, dim=0)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GNnMVrkNlU-6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jjquDUYFhDmf"
      },
      "outputs": [],
      "source": [
        "def calc_median_absolute_error(\n",
        "        t_azimuth,\n",
        "        t_elevation,\n",
        "        p_azimuth,\n",
        "        p_elevation\n",
        "):\n",
        "    \"\"\"\n",
        "    From Section 4: https://www.isca-archive.org/interspeech_2023/sarabia23_interspeech.html\n",
        "\n",
        "    Parameters:\n",
        "    t_azimuth (float): Azimuth angle of the true point in radians.\n",
        "    t_elevation (float): Elevation angle of the true point in radians.\n",
        "    p_azimuth (float): Azimuth angle of the predicted point in radians.\n",
        "    p_elevation (float): Elevation angle of the predicted point in radians.\n",
        "\n",
        "    Returns:\n",
        "    float: The angular distance in degrees.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calc angle error in radians\n",
        "    error_rad = torch.acos(\n",
        "        torch.sin(t_azimuth) * torch.sin(p_azimuth) +\n",
        "        torch.cos(t_azimuth) * torch.cos(p_azimuth) * torch.cos(t_elevation - p_elevation)\n",
        "    )\n",
        "\n",
        "    # Convert radians to degrees\n",
        "    error_deg = torch.rad2deg(error_rad)\n",
        "\n",
        "    # Calc median\n",
        "    median_error = torch.median(torch.abs(error_deg))\n",
        "\n",
        "    return median_error\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    train_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    for batch_idx, (active_input, reactive_input, labels) in enumerate(train_loader):\n",
        "        print(f\"Batch {batch_idx+1}/{len(train_loader)}\")\n",
        "\n",
        "        # Move data and labels to the device\n",
        "        active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(active_input, reactive_input)  # Pass both inputs to the model\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward(retain_graph=True)\n",
        "        # loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update training loss\n",
        "        train_loss = train_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "        true_azimuths.append(labels[:, 0].detach())\n",
        "        true_elevations.append(labels[:, 1].detach())\n",
        "        pred_azimuths.append(outputs[:, 0].detach())\n",
        "        pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate average loss over the dataset\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    return train_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ],
      "metadata": {
        "id": "-UHrSaeJgv_X"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed\n",
        "        for batch_idx, (active_input, reactive_input, labels) in enumerate(val_loader):\n",
        "            print(f\"Batch {batch_idx+1}/{len(val_loader)}\")\n",
        "\n",
        "            # Move the inputs and labels to the specified device\n",
        "            active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            outputs = model(active_input, reactive_input)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss = valid_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "            true_azimuths.append(labels[:, 0].detach())\n",
        "            true_elevations.append(labels[:, 1].detach())\n",
        "            pred_azimuths.append(outputs[:, 0].detach())\n",
        "            pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate the average loss over the dataset\n",
        "    valid_loss = valid_loss / len(val_loader.dataset)\n",
        "\n",
        "    return valid_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ],
      "metadata": {
        "id": "a3KVmEoEgyc8"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    test_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed during testing\n",
        "        for batch_idx, (active_input, reactive_input, labels) in enumerate(test_loader):\n",
        "            active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            outputs = model(active_input, reactive_input)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss = test_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "            # Optionally, accumulate metrics here\n",
        "            true_azimuths.append(labels[:, 0].detach())\n",
        "            true_elevations.append(labels[:, 1].detach())\n",
        "            pred_azimuths.append(outputs[:, 0].detach())\n",
        "            pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate the average loss over the dataset\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ],
      "metadata": {
        "id": "4xUJ05NBg27K"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = filter_data()\n",
        "train_data, val_data, test_data = split_data(data)\n",
        "\n",
        "train_dataset = AmbisonicDataset(data=train_data, config=config)\n",
        "val_dataset = AmbisonicDataset(data=val_data, config=config)\n",
        "test_dataset = AmbisonicDataset(data=test_data, config=config)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "1j_wWBIwg96V"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = config[\"device\"]\n",
        "model = SNN(config).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])"
      ],
      "metadata": {
        "id": "tvCPyyKuhBhi"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "\n",
        "    train_epoch_loss, train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation = train(model, train_loader, criterion, optimizer, device)\n",
        "    valid_epoch_loss, valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    valid_loss.append(valid_epoch_loss)\n",
        "\n",
        "    train_angle_error = calc_median_absolute_error(train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation)\n",
        "    valid_angle_error = calc_median_absolute_error(valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation)\n",
        "\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f} | Validation Loss: {valid_epoch_loss:.4f}\")\n",
        "    print(f\"Train Angle Error: {train_angle_error:.4f}° | Validation Angle Error: {valid_angle_error:.4f}°\\n\")\n",
        "\n",
        "test_loss, test_true_azimuth, test_true_elevation, test_pred_azimuth, test_pred_elevation = test(model, test_loader, criterion, device)\n",
        "test_angle_error = calc_median_absolute_error(test_true_azimuth, test_true_elevation, test_pred_azimuth, train_pred_elevation)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Angle Error: {test_angle_error:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KNpbk7KRh_BP",
        "outputId": "56cf0dc4-5882-4be2-fd2e-9befb55ce666"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Batch 1/457\n",
            "Batch 2/457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251: UserWarning: Error detected in NativeBatchNormBackward0. Traceback of forward call that caused the error:\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n",
            "    yield self.process_one()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n",
            "    runner = Runner(ctx_run, result, future, yielded)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-57-7c205d39f891>\", line 9, in <cell line: 6>\n",
            "    train_epoch_loss, train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation = train(model, train_loader, criterion, optimizer, device)\n",
            "  File \"<ipython-input-52-8a65125804cf>\", line 17, in train\n",
            "    outputs = model(active_input, reactive_input)  # Pass both inputs to the model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-50-d0fff8a42814>\", line 120, in forward\n",
            "    reactive_step = self.reactive_branch(reactive_in_permuted[step])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 215, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2478, in batch_norm\n",
            "    return torch.batch_norm(\n",
            " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [48]] is at version 3; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-7c205d39f891>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{config['num_epochs']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_true_azimuth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_true_elevation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred_azimuth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred_elevation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_true_azimuth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_true_elevation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pred_azimuth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pred_elevation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-8a65125804cf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Backward pass and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [48]] is at version 3; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
          ]
        }
      ]
    }
  ]
}