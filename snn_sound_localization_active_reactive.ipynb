{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mercadoerik1031/snn-sound-localization/blob/new_approach/snn_sound_localization_active_reactive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XWQW4Zj-Cwt",
        "outputId": "05460e67-2885-4e62-ab22-c06be51843b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0b1hp-2BO2-W"
      },
      "outputs": [],
      "source": [
        "! pip install snntorch optuna --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ogIzFacrhYZD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "from snntorch import utils\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import optuna\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nXnOrdx29edp"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # Filter Data\n",
        "    \"metadata_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/metadata.parquet\",\n",
        "    \"speech_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/ambisonics_lite\",\n",
        "    \"noise_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/data/noise_ambisonics_lite\",\n",
        "    \"is_lite_version\": True,\n",
        "\n",
        "    # Load Mix and STFT Audio\n",
        "    \"sr\": 16_000,\n",
        "    \"duration\": 0.5,\n",
        "    \"noise_ratio\": None, # Optional (Large #s make noise louder, Small #s make noise quieter)\n",
        "    \"n_fft\": 512,\n",
        "    \"hop_length\": 128,\n",
        "    \"win_length\": 512,\n",
        "\n",
        "    # Split Data\n",
        "    \"val_size\": 0.1,\n",
        "\n",
        "    # Dataset\n",
        "    \"num_steps\": 10,\n",
        "    \"batch_size\": 128,\n",
        "\n",
        "    # SNN\n",
        "    \"do\": 0.6,\n",
        "    \"beta\": 0.65,\n",
        "    \"beta_re\": 0.65,\n",
        "    \"beta_out\": 0.65,\n",
        "        # Active Branch\n",
        "    \"a_thresh1\": 10, \"a_thresh2\": 10, \"a_thresh3\": 10, \"a_thresh4\": 10,\n",
        "        # Reactive Branch\n",
        "    \"re_thresh1\": 10, \"re_thresh2\": 10, \"re_thresh3\": 10, \"re_thresh4\": 10,\n",
        "        # Output Layer\n",
        "    \"out_thresh1\": 10, \"out_thresh2\": 10,\n",
        "\n",
        "    # Training\n",
        "    \"num_epochs\": 10,\n",
        "    \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    \"lr\": 0.0003,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "T-yb90H8ry_W"
      },
      "outputs": [],
      "source": [
        "def filter_data(\n",
        "        metadata_pth=config[\"metadata_path\"],\n",
        "        speech_pth=config[\"speech_path\"],\n",
        "        noise_pth=config[\"noise_path\"],\n",
        "        lite_version=config[\"is_lite_version\"]\n",
        "):\n",
        "\n",
        "    metadata = pd.read_parquet(metadata_pth, engine=\"pyarrow\")\n",
        "    if lite_version:\n",
        "        metadata = metadata[metadata[\"lite_version\"] == True]\n",
        "\n",
        "    data = []\n",
        "    for _, row in metadata.iterrows():\n",
        "        sample = {\n",
        "            \"sample_id\": row[\"sample_id\"],\n",
        "            \"speech_path\": os.path.join(speech_pth, f\"{row['sample_id']:06}.flac\"),\n",
        "            \"noise_path\": os.path.join(noise_pth, f\"{row['sample_id']:06}.flac\") if noise_pth else None,\n",
        "            \"azimuth\": row[\"speech/azimuth\"],\n",
        "            \"elevation\": row[\"speech/elevation\"],\n",
        "            \"split\": row[\"split\"]\n",
        "        }\n",
        "        data.append(sample)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KzoFwpF4WMFP"
      },
      "outputs": [],
      "source": [
        "def split_data(data, val_size=config[\"val_size\"]):\n",
        "    train_data = [sample for sample in data if sample[\"split\"] == \"train\"]\n",
        "    test_data = [sample for sample in data if sample[\"split\"] == \"test\"]\n",
        "\n",
        "    train_data, val_data = train_test_split(train_data, test_size=val_size, random_state=42)\n",
        "\n",
        "    return train_data, val_data, test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gKUl7kz-uRUU"
      },
      "outputs": [],
      "source": [
        "def load_mix_and_stft_foa_audio(\n",
        "        speech_pth,\n",
        "        noise_pth=None,\n",
        "        sr=config[\"sr\"],\n",
        "        duration=config[\"duration\"],\n",
        "        noise_ratio=config[\"noise_ratio\"],\n",
        "        n_fft=config[\"n_fft\"],\n",
        "        hop_length=config[\"hop_length\"],\n",
        "        win_length=config[\"win_length\"],\n",
        "        device=config[\"device\"]\n",
        "):\n",
        "    # Function to load, resample, normalize, and trim/pad audio\n",
        "    # def preprocess_audio(audio_pth, sr, target_len, device):\n",
        "    #     audio, audio_sr = torchaudio.load(audio_pth)\n",
        "    #     audio = audio.to(device) # Move to GPU\n",
        "    #     if audio_sr != sr:\n",
        "    #         resample_transform = torchaudio.transforms.Resample(orig_freq=audio_sr, new_freq=sr)\n",
        "    #         audio = resample_transform(audio)\n",
        "    #     max_val = audio.abs().max()\n",
        "    #     if max_val > 0:  # Avoid division by zero\n",
        "    #         audio = audio / max_val\n",
        "    #     if audio.size(1) > target_len:\n",
        "    #         audio = audio[:, :target_len]\n",
        "    #     elif audio.size(1) < target_len:\n",
        "    #         padding_size = target_len - audio.size(1)\n",
        "    #         audio = torch.nn.functional.pad(audio, (0, padding_size), \"constant\", 0)\n",
        "    #     return audio\n",
        "\n",
        "    def preprocess_audio(audio_pth, sr, target_len, device):\n",
        "        num_frames = target_len\n",
        "        audio, audio_sr = torchaudio.load(audio_pth, frame_offset=0, num_frames=num_frames)\n",
        "        audio = audio.to(device)  # Move to GPU\n",
        "        if audio_sr != sr:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=audio_sr, new_freq=sr)\n",
        "            audio = resample_transform(audio)\n",
        "        max_val = audio.abs().max()\n",
        "        if max_val > 0:  # Avoid division by zero\n",
        "            audio = audio / max_val\n",
        "        # Check if padding is necessary (after resampling, the actual number of samples might change)\n",
        "        actual_len = audio.size(1)\n",
        "        if actual_len < target_len:\n",
        "            padding_size = target_len - actual_len\n",
        "            audio = torch.nn.functional.pad(audio, (0, padding_size), \"constant\", 0)\n",
        "        return audio\n",
        "\n",
        "    device = device\n",
        "    target_len = int(duration * sr)\n",
        "    speech_audio = preprocess_audio(speech_pth, sr, target_len, device)\n",
        "    should_renormalize = False\n",
        "\n",
        "    if noise_pth is not None:\n",
        "        noise_audio = preprocess_audio(noise_pth, sr, target_len, device)\n",
        "\n",
        "        if noise_ratio is not None:\n",
        "            # Adjust noise level relative to speech\n",
        "            noise_audio = noise_audio * noise_ratio\n",
        "            should_renormalize = True\n",
        "\n",
        "        # Mix speech and noise\n",
        "        mixed_audio = speech_audio + noise_audio\n",
        "    else:\n",
        "        mixed_audio = speech_audio\n",
        "\n",
        "    if should_renormalize:\n",
        "        # Re-normalize only if noise has been adjusted and mixed\n",
        "        max_val = mixed_audio.abs().max()\n",
        "        if max_val > 0:\n",
        "            mixed_audio = mixed_audio / max_val\n",
        "\n",
        "    # Move Window to device\n",
        "    window = torch.hann_window(win_length).to(device)\n",
        "\n",
        "    # Compute the STFT of the mixed audio\n",
        "    stft = torch.stft(mixed_audio,\n",
        "                      n_fft=n_fft,\n",
        "                      hop_length=hop_length,\n",
        "                      win_length=win_length,\n",
        "                      window=window,\n",
        "                      center=True,\n",
        "                      normalized=False,\n",
        "                      onesided=True,\n",
        "                      return_complex=True)\n",
        "\n",
        "    return stft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aMt7lTRd_lUw"
      },
      "outputs": [],
      "source": [
        "def compute_active_reactive_intensities(stft, rho=1.21, c=343):\n",
        "    \"\"\"\n",
        "    Compute active and reactive intensity vectors from STFT of 4-channel FOA audio.\n",
        "    Args:\n",
        "    - stft: STFT of the FOA audio with shape [4, Frequency Bins, Time Frames].\n",
        "    - rho: Mean density of air (in kg/m^3).\n",
        "    - c: Speed of sound in air (in m/s).\n",
        "\n",
        "    Returns:\n",
        "    - Ia: Active intensity vector.\n",
        "    - Ir: Reactive intensity vector.\n",
        "    \"\"\"\n",
        "    # Constants\n",
        "    three = torch.tensor(3.0, dtype=torch.float, device=stft.device)\n",
        "    normalization_factor = -1 / (rho * c * torch.sqrt(three))\n",
        "\n",
        "    # Extract channels\n",
        "    p = stft[0]  # Pressure (W channel)\n",
        "    vx = stft[1] * normalization_factor  # Velocity X\n",
        "    vy = stft[2] * normalization_factor  # Velocity Y\n",
        "    vz = stft[3] * normalization_factor  # Velocity Z\n",
        "\n",
        "    # Compute complex conjugate of pressure\n",
        "    p_star = torch.conj(p)\n",
        "\n",
        "    # Calculate active and reactive intensity vectors\n",
        "    Ia_x = torch.real(p_star * vx)\n",
        "    Ia_y = torch.real(p_star * vy)\n",
        "    Ia_z = torch.real(p_star * vz)\n",
        "\n",
        "    Ir_x = torch.imag(p_star * vx)\n",
        "    Ir_y = torch.imag(p_star * vy)\n",
        "    Ir_z = torch.imag(p_star * vz)\n",
        "\n",
        "    # Summing up the components to get total active and reactive intensities\n",
        "    # Ia = Ia_x + Ia_y + Ia_z  # Total active intensity\n",
        "    # Ir = Ir_x + Ir_y + Ir_z  # Total reactive intensity\n",
        "\n",
        "    # Create stack for each channel [3, num_samples, num_frames]\n",
        "    Ia = torch.stack((Ia_x, Ia_y, Ia_z), dim=0)\n",
        "    Ir = torch.stack((Ir_x, Ir_y, Ir_z), dim=0)\n",
        "\n",
        "    return Ia, Ir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JNsmExR-IyMJ"
      },
      "outputs": [],
      "source": [
        "class AmbisonicDataset(Dataset):\n",
        "    def __init__(self, data, config):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (list of dicts): Each dictionary contains paths and labels for a sample.\n",
        "            config (dict): Configuration dictionary including sample rate (sr), duration, etc.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.config = config\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Load and process ambisonic audio\n",
        "        speech_path = sample[\"speech_path\"]\n",
        "        noise_path = sample[\"noise_path\"] if \"noise_path\" in sample and sample[\"noise_path\"] is not None else None\n",
        "\n",
        "        stft_audio = load_mix_and_stft_foa_audio(\n",
        "            speech_path,\n",
        "            noise_pth=noise_path,\n",
        "            sr=self.config[\"sr\"],\n",
        "            duration=self.config[\"duration\"],\n",
        "            noise_ratio=self.config[\"noise_ratio\"],\n",
        "            n_fft=self.config[\"n_fft\"],\n",
        "            hop_length=self.config[\"hop_length\"],\n",
        "            win_length=self.config[\"win_length\"],\n",
        "            device=self.config[\"device\"]\n",
        "        )\n",
        "\n",
        "        # Compute active and reactive intensities\n",
        "        Ia, Ir = compute_active_reactive_intensities(stft_audio, rho=1.21, c=343)\n",
        "\n",
        "        # Generate Spike Trains\n",
        "        spikes_Ia = spikegen.rate(Ia, num_steps=self.config[\"num_steps\"])\n",
        "        spikes_Ir = spikegen.rate(Ir, num_steps=self.config[\"num_steps\"])\n",
        "\n",
        "        azimuth = sample['azimuth']\n",
        "        elevation = sample['elevation']\n",
        "        label = torch.tensor([azimuth, elevation], dtype=torch.float)\n",
        "\n",
        "        #print(f\"stft_audio.shape: {stft_audio.shape}\\nIa.shape:{Ia.shape}, Ir.shape: {Ir.shape}\\nspikes_Ia.shape: {spikes_Ia.shape}, spikes_Ir.shape: {spikes_Ir.shape}\\nlabel.shape: {label.shape}\")\n",
        "\n",
        "        return spikes_Ia, spikes_Ir, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6uLnmiEuayje"
      },
      "outputs": [],
      "source": [
        "# for spikes_active, spikes_reactive, lables in train_loader:\n",
        "#     print(spikes_active.shape)\n",
        "#     print(spikes_reactive.shape)\n",
        "#     print(lables.shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0EpXey0icmpI"
      },
      "outputs": [],
      "source": [
        "# for spikes_Ia, spikes_Ir, _ in train_loader:\n",
        "#     spikes_per_step_Ia = spikes_Ia.sum(dim=[2, 3])\n",
        "#     spikes_per_step_Ir = spikes_Ir.sum(dim=[2, 3])\n",
        "#     print(f\"Spikes in Ia: {spikes_per_step_Ia}\")\n",
        "#     print(f\"Spikes in Ir: {spikes_per_step_Ir}\")\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GNnMVrkNlU-6"
      },
      "outputs": [],
      "source": [
        "class SNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(SNN, self).__init__()\n",
        "        self.num_steps = config[\"num_steps\"]\n",
        "        self.beta = config[\"beta\"]\n",
        "        self.beta_re = config[\"beta_re\"]\n",
        "        self.beta_out = config[\"beta_out\"]\n",
        "\n",
        "        # Active Thresholds\n",
        "        self.a_thr1 = config[\"a_thresh1\"]\n",
        "        self.a_thr2 = config[\"a_thresh2\"]\n",
        "        self.a_thr3 = config[\"a_thresh3\"]\n",
        "        self.a_thr4 = config[\"a_thresh4\"]\n",
        "\n",
        "        # Reactive Thresholds\n",
        "        self.re_thr1 = config[\"re_thresh1\"]\n",
        "        self.re_thr2 = config[\"re_thresh2\"]\n",
        "        self.re_thr3 = config[\"re_thresh3\"]\n",
        "        self.re_thr4 = config[\"re_thresh4\"]\n",
        "\n",
        "        ## FC Layer\n",
        "        self.beta_out = config[\"beta_out\"]\n",
        "        self.out_thr1 = config[\"out_thresh1\"]\n",
        "        self.out_thr2 = config[\"out_thresh2\"]\n",
        "        self.do = config[\"do\"]\n",
        "\n",
        "        # Define the active branch with LIF neurons\n",
        "        self.active_branch = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr1, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(6, 12, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr2, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(12, 24, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr3, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(24, 48, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta, threshold=self.a_thr4, init_hidden=True),\n",
        "        )\n",
        "\n",
        "        self.reactive_branch = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta_re, threshold=self.re_thr1, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(6, 12, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta_re, threshold=self.re_thr2, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(12, 24, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta_re, threshold=self.re_thr3, init_hidden=True),\n",
        "\n",
        "            nn.Conv2d(24, 48, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            nn.MaxPool2d(2),\n",
        "            snn.Leaky(beta=self.beta_re, threshold=self.re_thr4, init_hidden=True),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(4608, 512),\n",
        "            snn.Leaky(beta=self.beta_out, threshold=self.out_thr1, init_hidden=True),\n",
        "            nn.Linear(512, 256),\n",
        "            snn.Leaky(beta=self.beta_out, threshold=self.out_thr2, init_hidden=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Dropout(p=self.do),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    # def forward(self, active_input, reactive_input):\n",
        "    #     active_in_permuted = active_input.permute(1, 0, 2, 3, 4) # [num_step, batch_size, channels, num_samples, num_frames]\n",
        "    #     reactive_in_permuted = reactive_input.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "    #     active_outputs = []\n",
        "    #     reactive_outputs = []\n",
        "\n",
        "    #     for step in range(active_in_permuted.size(0)):\n",
        "    #         active_step = self.active_branch(active_in_permuted[step])\n",
        "    #         reactive_step = self.reactive_branch(reactive_in_permuted[step])\n",
        "\n",
        "    #         active_outputs.append(active_step)\n",
        "    #         reactive_outputs.append(reactive_step)\n",
        "\n",
        "    #     # Stack the outputs across time steps to form tensors of shape [num_steps, batch_size, channels, height, width]\n",
        "    #     active_stacked = torch.stack(active_outputs, dim=0)\n",
        "    #     reactive_stacked = torch.stack(reactive_outputs, dim=0)\n",
        "\n",
        "    #     # Aggregate across time steps, e.g., by taking the mean or sum\n",
        "    #     active_agg = torch.mean(active_stacked, dim=0)\n",
        "    #     reactive_agg = torch.mean(reactive_stacked, dim=0)\n",
        "\n",
        "    #     # Flatten and concatenate the aggregated outputs for the final MLP\n",
        "    #     active_flat = active_agg.view(active_agg.size(0), -1)\n",
        "    #     reactive_flat = reactive_agg.view(reactive_agg.size(0), -1)\n",
        "    #     combined = torch.cat((active_flat, reactive_flat), dim=1)\n",
        "\n",
        "    #     output = self.fc(combined)\n",
        "\n",
        "    #     return output\n",
        "\n",
        "    def forward(self, active_input, reactive_input):\n",
        "        step_outputs = []\n",
        "        permute_active = active_input.permute(1, 0, 2, 3, 4)\n",
        "        permute_reactive = reactive_input.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        if permute_active.size(0) != permute_reactive.size(0):\n",
        "            raise ValueError(\"The Tme Steps from active and reactive Do NOT Match\")\n",
        "\n",
        "        for step in range(permute_active.size(0)):\n",
        "\n",
        "            current_active = permute_active[step]\n",
        "            current_reactive = permute_reactive[step]\n",
        "\n",
        "            # Process inputs through active and reactive branches\n",
        "            active_out = self.active_branch(current_active)\n",
        "            reactive_out = self.reactive_branch(current_reactive)\n",
        "\n",
        "            # Flatten and combine the outputs\n",
        "            combined = torch.cat((active_out, reactive_out), dim=1)\n",
        "            combined = combined.view(combined.size(0), -1)\n",
        "\n",
        "            fc_out = self.fc(combined)\n",
        "\n",
        "            step_outputs.append(fc_out)\n",
        "\n",
        "        tensor_out = torch.stack(step_outputs, dim=0)\n",
        "        output = torch.mean(tensor_out, dim=0)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jjquDUYFhDmf"
      },
      "outputs": [],
      "source": [
        "def calc_median_absolute_error(\n",
        "        t_azimuth,\n",
        "        t_elevation,\n",
        "        p_azimuth,\n",
        "        p_elevation\n",
        "):\n",
        "    \"\"\"\n",
        "    From Section 4: https://www.isca-archive.org/interspeech_2023/sarabia23_interspeech.html\n",
        "\n",
        "    Parameters:\n",
        "    t_azimuth (float): Azimuth angle of the true point in radians.\n",
        "    t_elevation (float): Elevation angle of the true point in radians.\n",
        "    p_azimuth (float): Azimuth angle of the predicted point in radians.\n",
        "    p_elevation (float): Elevation angle of the predicted point in radians.\n",
        "\n",
        "    Returns:\n",
        "    float: The angular distance in degrees.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calc angle error in radians\n",
        "    error_rad = torch.acos(\n",
        "        torch.sin(t_azimuth) * torch.sin(p_azimuth) +\n",
        "        torch.cos(t_azimuth) * torch.cos(p_azimuth) * torch.cos(t_elevation - p_elevation)\n",
        "    )\n",
        "\n",
        "    # Convert radians to degrees\n",
        "    error_deg = torch.rad2deg(error_rad)\n",
        "\n",
        "    # Calc median\n",
        "    median_error = torch.median(torch.abs(error_deg))\n",
        "\n",
        "    return median_error\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-UHrSaeJgv_X"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    train_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    for batch_idx, (active_input, reactive_input, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        #print(f\"Batch {batch_idx+1}/{len(train_loader)}\")\n",
        "\n",
        "        # Move data and labels to the device\n",
        "        active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "        # Reset branches - required for init_hidden=True\n",
        "        utils.reset(model.active_branch)\n",
        "        utils.reset(model.reactive_branch)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(active_input, reactive_input)  # Pass both inputs to the model\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update training loss\n",
        "        train_loss = train_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "        true_azimuths.append(labels[:, 0].detach())\n",
        "        true_elevations.append(labels[:, 1].detach())\n",
        "        pred_azimuths.append(outputs[:, 0].detach())\n",
        "        pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate average loss over the dataset\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    return train_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "a3KVmEoEgyc8"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed\n",
        "        for batch_idx, (active_input, reactive_input, labels) in enumerate(tqdm(val_loader, desc=\"Validation\")):\n",
        "            #print(f\"Batch {batch_idx+1}/{len(val_loader)}\")\n",
        "\n",
        "            # Move the inputs and labels to the specified device\n",
        "            active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            outputs = model(active_input, reactive_input)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss = valid_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "            true_azimuths.append(labels[:, 0].detach())\n",
        "            true_elevations.append(labels[:, 1].detach())\n",
        "            pred_azimuths.append(outputs[:, 0].detach())\n",
        "            pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate the average loss over the dataset\n",
        "    valid_loss = valid_loss / len(val_loader.dataset)\n",
        "\n",
        "    return valid_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4xUJ05NBg27K"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    test_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed during testing\n",
        "        for batch_idx, (active_input, reactive_input, labels) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
        "            active_input, reactive_input, labels = active_input.to(device), reactive_input.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass: compute the model output\n",
        "            outputs = model(active_input, reactive_input)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss = test_loss + loss.item() * active_input.size(0)\n",
        "\n",
        "            # Optionally, accumulate metrics here\n",
        "            true_azimuths.append(labels[:, 0].detach())\n",
        "            true_elevations.append(labels[:, 1].detach())\n",
        "            pred_azimuths.append(outputs[:, 0].detach())\n",
        "            pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    # Calculate the average loss over the dataset\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1j_wWBIwg96V"
      },
      "outputs": [],
      "source": [
        "# data = filter_data()\n",
        "# train_data, val_data, test_data = split_data(data)\n",
        "\n",
        "# train_dataset = AmbisonicDataset(data=train_data, config=config)\n",
        "# val_dataset = AmbisonicDataset(data=val_data, config=config)\n",
        "# test_dataset = AmbisonicDataset(data=test_data, config=config)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jub3xhObR8RM"
      },
      "outputs": [],
      "source": [
        "# print(f\"len(train_dataset: {len(train_dataset)})\")\n",
        "# print(f\"len(val_dataset: {len(val_dataset)})\")\n",
        "# print(f\"len(test_dataset: {len(test_dataset)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tvCPyyKuhBhi"
      },
      "outputs": [],
      "source": [
        "# device = config[\"device\"]\n",
        "\n",
        "# model = SNN(config)\n",
        "# if torch.cuda.device_count() > 1:\n",
        "#     print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
        "#     model = torch.nn.DataParallel(model)\n",
        "# model.to(device)\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "eUbG7PSIQBSV"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Hyperparameters to optimize\n",
        "    config[\"a_thresh1\"] = trial.suggest_float(\"a_thresh1\", 1, 30)\n",
        "    config[\"a_thresh2\"] = trial.suggest_float(\"a_thresh2\", 1, 30)\n",
        "    config[\"a_thresh3\"] = trial.suggest_float(\"a_thresh3\", 1, 30)\n",
        "    config[\"a_thresh4\"] = trial.suggest_float(\"a_thresh4\", 1, 30)\n",
        "    config[\"beta\"] = trial.suggest_float(\"beta\", 0.1, 0.9)\n",
        "\n",
        "\n",
        "    config[\"re_thresh1\"] = trial.suggest_float(\"re_thresh1\", 1, 30)\n",
        "    config[\"re_thresh2\"] = trial.suggest_float(\"re_thresh2\", 1, 30)\n",
        "    config[\"re_thresh3\"] = trial.suggest_float(\"re_thresh3\", 1, 30)\n",
        "    config[\"re_thresh4\"] = trial.suggest_float(\"re_thresh4\", 1, 30)\n",
        "    config[\"beta_re\"] = trial.suggest_float(\"beta_re\", 0.1, 0.9)\n",
        "\n",
        "    config[\"out_thresh1\"] = trial.suggest_float(\"out_thresh1\", 1, 30)\n",
        "    config[\"out_thresh2\"] = trial.suggest_float(\"out_thresh2\", 1, 30)\n",
        "    config[\"beta_out\"] = trial.suggest_float(\"beta_re\", 0.1, 0.9)\n",
        "\n",
        "    config[\"do\"] = trial.suggest_float(\"do\", 0.1, 0.9)\n",
        "    config[\"num_steps\"] = trial.suggest_int(\"num_steps\", 5, 20)\n",
        "    config[\"lr\"] = trial.suggest_float(\"lr\", 1e-10, 1e-3)\n",
        "\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SNN(config)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(f\"Using {torch.device_count()} GPUs.\")\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    data = filter_data()\n",
        "    train_data, val_data, test_data = split_data(data)\n",
        "\n",
        "    train_dataset = AmbisonicDataset(data=train_data, config=config)\n",
        "    val_dataset = AmbisonicDataset(data=val_data, config=config)\n",
        "    test_dataset = AmbisonicDataset(data=test_data, config=config)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, drop_last=True)\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        train(model, train_loader, criterion, optimizer, device)\n",
        "        valid_epoch_loss, valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation = validate(model, val_loader, criterion, device)\n",
        "        valid_angle_error = calc_median_absolute_error(valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation)\n",
        "\n",
        "    return valid_angle_error.item()  # Optuna minimizes this value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZT8BYIRQQODD"
      },
      "outputs": [],
      "source": [
        "def run_study():\n",
        "    study_name = \"snn_study\"\n",
        "    storage_name = \"sqlite:///{}.db\".format(study_name)\n",
        "\n",
        "    study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Save the study to disk\n",
        "    study.trials_dataframe().to_csv(\"study_results.csv\")\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"    Value: \", trial.value)\n",
        "    print(\"    Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"      {key}: {value}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aTJaM96QRGA",
        "outputId": "2be71d01-e0a6-438c-befa-fbc322a7e1e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-02-26 22:05:18,035] A new study created in RDB with name: snn_study\n",
            "Training: 100%|██████████| 120/120 [3:14:58<00:00, 97.49s/it]\n",
            "Validation: 100%|██████████| 13/13 [21:13<00:00, 97.96s/it]\n",
            "Training: 100%|██████████| 120/120 [2:17:30<00:00, 68.75s/it]\n",
            "Validation:  31%|███       | 4/13 [06:15<14:01, 93.53s/it]"
          ]
        }
      ],
      "source": [
        "run_study()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNpbk7KRh_BP"
      },
      "outputs": [],
      "source": [
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# train_loss = []\n",
        "# valid_loss = []\n",
        "\n",
        "# for epoch in range(config[\"num_epochs\"]):\n",
        "#     print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "\n",
        "#     train_epoch_loss, train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation = train(model, train_loader, criterion, optimizer, device)\n",
        "#     valid_epoch_loss, valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation = validate(model, val_loader, criterion, device)\n",
        "\n",
        "#     train_loss.append(train_epoch_loss)\n",
        "#     valid_loss.append(valid_epoch_loss)\n",
        "\n",
        "#     train_angle_error = calc_median_absolute_error(train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation)\n",
        "#     valid_angle_error = calc_median_absolute_error(valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation)\n",
        "\n",
        "#     print(f\"Train Loss: {train_epoch_loss:.4f} | Validation Loss: {valid_epoch_loss:.4f}\")\n",
        "#     print(f\"Train Angle Error: {train_angle_error:.4f}° | Validation Angle Error: {valid_angle_error:.4f}°\\n\")\n",
        "\n",
        "# test_loss, test_true_azimuth, test_true_elevation, test_pred_azimuth, test_pred_elevation = test(model, test_loader, criterion, device)\n",
        "# test_angle_error = calc_median_absolute_error(test_true_azimuth, test_true_elevation, test_pred_azimuth, train_pred_elevation)\n",
        "\n",
        "# print(f\"Test Loss: {test_loss:.4f}\")\n",
        "# print(f\"Test Angle Error: {test_angle_error:.4f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNWoncUlt9c3PDUylcH4G0q",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
