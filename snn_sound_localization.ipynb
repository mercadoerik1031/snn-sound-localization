{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mercadoerik1031/snn-sound-localization/blob/write_to_disk/snn_sound_localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwwANRnDjZcI"
      },
      "source": [
        "#**SNN Sounnd Localization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqx8gNURjTAG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E85Rvhujlsd"
      },
      "source": [
        "# Pip Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vmiMyIQVT8gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125a7b72-a365-4ada-a0e9-0dd7ddebe69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m417.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vApVhQVXjPjs"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FkLpfcRNSoFm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from snntorch import spikegen\n",
        "import gc\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6vcXjzWStl0",
        "outputId": "550234d8-9957-4cb8-e5a1-57a9a2555770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qXpXNVaSoFq"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OsF-UzANSoFr"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # Google Colab Path\n",
        "    \"metadata_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/metadata.parquet\",\n",
        "    \"ambisonics_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/ambisonics_sample\",\n",
        "    \"noise_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/noise_ambisonics_sample\",\n",
        "    \"output_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/preprocessed_samples\",\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"batch_size_pre\": 32,\n",
        "    \"sr\": 16000,\n",
        "\n",
        "    \"time_based_encoding\": True,\n",
        "    \"num_steps\": 10,\n",
        "    \"max_rate\": 10,\n",
        "    \"noise\": True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter Data"
      ],
      "metadata": {
        "id": "Y9C0Ea6NkSQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data(metadata_path=config[\"metadata_path\"], ambisonics_path=config[\"ambisonics_path\"], noise_path=config[\"noise_path\"]):\n",
        "  # Load metadata\n",
        "  metadata = pd.read_parquet(metadata_path, engine=\"pyarrow\")\n",
        "\n",
        "  # Get lists of all files in directories\n",
        "  ambisonic_files = [f for f in os.listdir(ambisonics_path) if os.path.isfile(os.path.join(ambisonics_path, f))]\n",
        "  noise_files = [f for f in os.listdir(noise_path) if os.path.isfile(os.path.join(noise_path, f))]\n",
        "\n",
        "  # Extract sample ids from filenames and filter metadata\n",
        "  sample_ids = [int(f.split(\".\")[0].lstrip(\"0\") or 0) for f in ambisonic_files]\n",
        "  filtered_metadata = metadata[metadata[\"sample_id\"].isin(sample_ids)]\n",
        "\n",
        "  # Create full file paths\n",
        "  ambisonic_files = [os.path.join(ambisonics_path, f) for f in ambisonic_files]\n",
        "  noise_files = [os.path.join(noise_path, f) for f in noise_files]\n",
        "\n",
        "  return filtered_metadata, ambisonic_files, noise_files\n"
      ],
      "metadata": {
        "id": "BRO_nYuFkUuK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toiYb9hzSoFv"
      },
      "source": [
        "# Preprocess Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87GaxiccSoFw"
      },
      "source": [
        "## Normalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(audio_data, device=config[\"device\"]):\n",
        "  audio_data = audio_data.to(device)\n",
        "  return (audio_data - audio_data.min()) / (audio_data.max() - audio_data.min())\n"
      ],
      "metadata": {
        "id": "615Sk9vy0GER"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASBLdCnSoFx"
      },
      "source": [
        "## Rate Based Encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_based_encoding(audio_data, max_rate=config[\"max_rate\"], num_steps=config[\"num_steps\"], device=config[\"device\"]):\n",
        "    if audio_data is None:\n",
        "        raise ValueError(\"Input data is None.\")\n",
        "\n",
        "    # Check if audio_data is already a tensor, if not convert it\n",
        "    if not isinstance(audio_data, torch.Tensor):\n",
        "        audio_data = torch.tensor(audio_data, device=device)\n",
        "\n",
        "    audio_data = audio_data.float().to(device)\n",
        "\n",
        "    normalized_data = normalize(audio_data)\n",
        "\n",
        "    spike_rates = normalized_data * max_rate\n",
        "\n",
        "    spike_train = spikegen.rate(spike_rates, num_steps=num_steps)\n",
        "\n",
        "    return spike_train\n",
        "\n"
      ],
      "metadata": {
        "id": "0WkaWpuW0hzE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPJSC6NOSoFy"
      },
      "source": [
        "## Time Based Encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_based_encoding(audio_data, num_steps=config[\"num_steps\"], device=config[\"device\"]):\n",
        "    if audio_data is None:\n",
        "        raise ValueError(\"Input data is None.\")\n",
        "\n",
        "    # Check if audio_data is already a tensor, if not convert it\n",
        "    if not isinstance(audio_data, torch.Tensor):\n",
        "        audio_data = torch.tensor(audio_data, device=device)\n",
        "\n",
        "    audio_data = audio_data.float().to(device)\n",
        "\n",
        "    normalized_data = normalize(audio_data)\n",
        "\n",
        "    spike_times = torch.where(normalized_data > 0.5, 1, 0)\n",
        "\n",
        "    spike_train = spikegen.latency(spike_times, num_steps=num_steps, bypass=True)\n",
        "\n",
        "    return spike_train\n"
      ],
      "metadata": {
        "id": "qkLw8tJk0lRx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Function"
      ],
      "metadata": {
        "id": "uCy0L63q5ReB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(ambisonic_file, noise_file, duration, device=config[\"device\"], sr=config[\"sr\"]):\n",
        "    # Load ambisonic audio directly to GPU if possible\n",
        "    audio = torch.tensor(librosa.load(ambisonic_file, sr=sr, mono=False, duration=duration)[0], device=device)\n",
        "    length = int(np.round(duration * sr))\n",
        "\n",
        "    # Pad Ambisonic File\n",
        "    padded_ambisonic = torch.nn.functional.pad(audio, (0, max(0, length - audio.shape[1])))\n",
        "\n",
        "    # Combine Noise (Optional)\n",
        "    if config[\"noise\"] and noise_file:\n",
        "        # Load Noise File\n",
        "        noise_audio = torch.tensor(librosa.load(noise_file, sr=sr, mono=False, duration=duration)[0], device=device)\n",
        "\n",
        "        # Pad Noise File\n",
        "        padded_noise = torch.nn.functional.pad(noise_audio, (0, max(0, length - noise_audio.shape[1])))\n",
        "\n",
        "        # Combine Ambisonic & Noise\n",
        "        combined_audio = padded_ambisonic + padded_noise\n",
        "    else:\n",
        "        combined_audio = padded_ambisonic\n",
        "\n",
        "    # Processed_audio should be processed on GPU\n",
        "    spike_trains = time_based_encoding(combined_audio) if config[\"time_based_encoding\"] else rate_based_encoding(combined_audio)\n",
        "\n",
        "    return spike_trains\n",
        "\n"
      ],
      "metadata": {
        "id": "LPJVTkMLBqYM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process & Save Batches"
      ],
      "metadata": {
        "id": "8_QbDmF6AcA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch_ambisonic_files, batch_noise_files, batch_metadata, output_path, duration, sr, batch_id):\n",
        "    processed_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Process each file in the batch\n",
        "    for ambisonic_file, noise_file, meta_row in zip(batch_ambisonic_files, batch_noise_files, batch_metadata.itertuples()):\n",
        "        spike_trains = preprocess(ambisonic_file, noise_file, duration)\n",
        "        processed_data.append(spike_trains.cpu())\n",
        "\n",
        "        labels.append({\n",
        "            'sample_id': meta_row.sample_id,\n",
        "            'split': meta_row.split,\n",
        "            'azimuth': batch_metadata.at[meta_row.Index, 'speech/azimuth'],\n",
        "            'elevation': batch_metadata.at[meta_row.Index, 'speech/elevation']\n",
        "        })\n",
        "\n",
        "    # Save processed data and labels\n",
        "    batch_data_filename = f'processed_batch_{batch_id}.pt'\n",
        "    batch_labels_filename = f'labels_batch_{batch_id}.csv'\n",
        "    torch.save(torch.stack(processed_data), os.path.join(output_path, batch_data_filename))\n",
        "    pd.DataFrame(labels).to_csv(os.path.join(output_path, batch_labels_filename), index=False)\n",
        "\n",
        "    print(f\"Batch {batch_id} processed and saved.\")\n",
        "\n",
        "def parallel_process_batches(metadata, ambisonic_files, noise_files, duration, batch_size=config[\"batch_size_pre\"], output_path=config[\"output_path\"], sr=config[\"sr\"], max_workers=4):\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = []\n",
        "        for i in range(0, len(ambisonic_files), batch_size):\n",
        "            batch_ambisonic_files = ambisonic_files[i:i+batch_size]\n",
        "            batch_noise_files = noise_files[i:i+batch_size]\n",
        "            batch_metadata = metadata.iloc[i:i+batch_size]\n",
        "\n",
        "            # Submit batch processing jobs to the executor\n",
        "            futures.append(executor.submit(process_batch, batch_ambisonic_files, batch_noise_files, batch_metadata, output_path, duration, sr, i // batch_size))\n",
        "\n",
        "        # Wait for all futures to complete\n",
        "        for future in futures:\n",
        "            future.result()\n",
        "\n",
        "    print(\"All batches processed and saved in parallel.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dGGvuCn6wyES"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoaders"
      ],
      "metadata": {
        "id": "u5Bco7m3A0SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoundLocalizationDataset(Dataset):\n",
        "    def __init__(self, base_path, total_batches):\n",
        "        \"\"\"\n",
        "        base_path: Path where batch data and label files are stored.\n",
        "        total_batches: Total number of batches.\n",
        "        \"\"\"\n",
        "        self.base_path = base_path\n",
        "        self.total_batches = total_batches\n",
        "        self.labels_cache = {}  # Cache to store loaded labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_batches * batch_size  # Assuming each batch has 'batch_size' samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_id = idx // batch_size\n",
        "        local_idx = idx % batch_size\n",
        "\n",
        "        # Load batch data\n",
        "        data_path = f'{self.base_path}/processed_batch_{batch_id}.pt'\n",
        "        batch_data = torch.load(data_path)\n",
        "\n",
        "        # Load labels for the batch if not already loaded\n",
        "        if batch_id not in self.labels_cache:\n",
        "            label_path = f'{self.base_path}/labels_batch_{batch_id}.csv'\n",
        "            self.labels_cache[batch_id] = pd.read_csv(label_path)\n",
        "\n",
        "        # Fetch label\n",
        "        label_df = self.labels_cache[batch_id]\n",
        "        label = label_df.iloc[local_idx][['azimuth', 'elevation']].values.astype('float32')\n",
        "\n",
        "        sample = batch_data[local_idx]\n",
        "\n",
        "        return sample, label\n"
      ],
      "metadata": {
        "id": "AAtHCdoUA2WP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "87abc899-4001-4267-eb14-e495d976f0e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-33c1711bf99a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSoundLocalizationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \"\"\"\n\u001b[1;32m      4\u001b[0m         \u001b[0mbase_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0mare\u001b[0m \u001b[0mstored\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtotal_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTotal\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have 32 batches, for example\n",
        "total_batches = 32\n",
        "base_path = \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/preprocessed_samples\"\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = SoundLocalizationDataset(base_path, total_batches)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 32  # Update this as per your requirement\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Accessing the first batch data and labels\n",
        "first_batch_data, first_batch_labels = next(iter(data_loader))\n",
        "print(\"First batch data:\", first_batch_data)\n",
        "print(\"First batch labels:\", first_batch_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "VwUE0tYb8q0E",
        "outputId": "02d3c686-a021-4c5f-9c2b-681bea8eb216"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 processed and saved.\n",
            "Batch 1 processed and saved.\n",
            "Batch 2 processed and saved.\n",
            "Batch 3 processed and saved.\n",
            "Batch 4 processed and saved.\n",
            "Batch 5 processed and saved.\n",
            "Batch 6 processed and saved.\n",
            "Batch 7 processed and saved.\n",
            "Batch 8 processed and saved.\n",
            "Batch 9 processed and saved.\n",
            "Batch 10 processed and saved.\n",
            "Batch 11 processed and saved.\n",
            "Batch 12 processed and saved.\n",
            "Batch 13 processed and saved.\n",
            "Batch 14 processed and saved.\n",
            "Batch 15 processed and saved.\n",
            "Batch 16 processed and saved.\n",
            "Batch 17 processed and saved.\n",
            "Batch 18 processed and saved.\n",
            "Batch 19 processed and saved.\n",
            "Batch 20 processed and saved.\n",
            "Batch 21 processed and saved.\n",
            "Batch 22 processed and saved.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/preprocessed_samples'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ec7e1967d8b0>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mlabels_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m  \u001b[0;31m# Assuming labels are ordered as per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/preprocessed_samples'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch_data, first_batch_labels = next(iter(train_loader))\n",
        "print(\"First batch data:\", first_batch_data)\n",
        "print(\"First batch labels:\", first_batch_labels)"
      ],
      "metadata": {
        "id": "_6ifjzayH-HW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}