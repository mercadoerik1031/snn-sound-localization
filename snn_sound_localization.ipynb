{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"metadata_path\": \"spatial_librispeech_sample/metadata.parquet\",\n",
    "    \"ambisonics_path\": \"c:\\\\Users\\\\merca\\\\OneDrive\\\\Documents\\\\MyFiles\\\\Code\\\\masters_project\\\\spatial_librispeech_sample\\\\ambisonics_sample\",\n",
    "    \n",
    "    \"time_based_encoding\": True,\n",
    "    \"num_steps\": 100,\n",
    "    \"max_rate\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the metadata file\n",
    "metadata_path = config[\"metadata_path\"]\n",
    "\n",
    "# Load the metadata file\n",
    "metadata = pd.read_parquet(metadata_path, engine=\"pyarrow\")\n",
    "\n",
    "# Path to the ambisonics folder\n",
    "ambisonics_path = config[\"ambisonics_path\"]\n",
    "\n",
    "# List all files in the ambisonics folder\n",
    "ambisonics_files = [f for f in os.listdir(ambisonics_path) if os.path.isfile(os.path.join(ambisonics_path, f))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip 0s from filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 47)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ids = []\n",
    "\n",
    "for file_name in ambisonics_files:\n",
    "    number, _ = file_name.split(\".\")\n",
    "    number.lstrip(\"0\")\n",
    "    \n",
    "    if not number:\n",
    "        number = 0\n",
    "        \n",
    "    sample_ids.append(int(number))\n",
    "\n",
    "filtered_metadata = metadata[metadata[\"sample_id\"].isin(sample_ids)]\n",
    "filtered_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cochlear Filer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cochlear_filter(channel_data, sr):\n",
    "    pass\n",
    "    # return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return (data - data.min()) / (data.max() - data.min()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate Based Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_based_encoding(data, max_rate=100, num_steps=100):\n",
    "    data_tensor = torch.from_numpy(data).float()\n",
    "    \n",
    "    normalized_data = normalize(data)\n",
    "    \n",
    "    spike_rates = normalized_data * max_rate\n",
    "    \n",
    "    spike_train = spikegen.rate(spike_rates, num_steps= num_steps)\n",
    "    \n",
    "    return spike_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Based Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_encoding(data, num_steps=100):\n",
    "    data_tensor = torch.from_numpy(data).float()\n",
    "    \n",
    "    normalized_data = normalize(data)\n",
    "    \n",
    "    spike_times = torch.where(normalized_data > 0.5, 1, 0)\n",
    "    \n",
    "    spike_trains = spikegen.latency(spike_times, num_steps=num_steps)\n",
    "    \n",
    "    return spike_trains\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(filepath, max_duration):\n",
    "    \"\"\"\n",
    "    W: Omnidirectional\n",
    "    X: Front - Back\n",
    "    Y: Left - Right\n",
    "    Z: Top - Bottom\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(filepath, sr=None, mono=False)\n",
    "    # print(f\"Original shape: {audio.shape}, Sampling rate: {sr}\")\n",
    "\n",
    "    max_length = int(max_duration * sr)\n",
    "    # print(f\"Max length in samples: {max_length}\")\n",
    "\n",
    "    padded_audio = librosa.util.fix_length(data=audio, size=max_length)\n",
    "    # print(f\"Padded shape: {padded_audio.shape}\")\n",
    "\n",
    "    W, X, Y, Z = padded_audio[0], padded_audio[1], padded_audio[2], padded_audio[3]\n",
    "    \n",
    "    processed_W = cochlear_filter(W, sr)\n",
    "    processed_X = cochlear_filter(X, sr)\n",
    "    processed_Y = cochlear_filter(Y, sr)\n",
    "    processed_Z = cochlear_filter(Z, sr)\n",
    "\n",
    "    if config[\"time_based_encoding\"]:\n",
    "        spike_trains_W = time_based_encoding(processed_W, config[\"num_steps\"])\n",
    "        spike_trains_X = time_based_encoding(processed_X, config[\"num_steps\"])\n",
    "        spike_trains_Y = time_based_encoding(processed_Y, config[\"num_steps\"])\n",
    "        spike_trains_Z = time_based_encoding(processed_Z, config[\"num_steps\"])\n",
    "    else:\n",
    "        spike_trains_W = rate_based_encoding(processed_W, config['max_rate'], config['num_steps'])\n",
    "        spike_trains_X = rate_based_encoding(processed_X, config['max_rate'], config['num_steps'])\n",
    "        spike_trains_Y = rate_based_encoding(processed_Y, config['max_rate'], config['num_steps'])\n",
    "        spike_trains_Z = rate_based_encoding(processed_Z, config['max_rate'], config['num_steps'])\n",
    "\n",
    "    return spike_trains_W, spike_trains_X, spike_trains_Y, spike_trains_Z\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_W, spikes_X, spikes_Y, spikes_Z = preprocess_audio(os.path.join(ambisonics_path, ambisonics_files[0]), filtered_metadata[\"audio_info/duration\"].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
