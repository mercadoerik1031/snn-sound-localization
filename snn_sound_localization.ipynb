{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mercadoerik1031/snn-sound-localization/blob/main/snn_sound_localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwwANRnDjZcI"
      },
      "source": [
        "#**SNN Sounnd Localization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqx8gNURjTAG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E85Rvhujlsd"
      },
      "source": [
        "# Pip Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vmiMyIQVT8gh",
        "outputId": "134455ee-ee22-49a6-9b2f-09be84e2e252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for brian2hears (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install snntorch brian2 brian2hears --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vApVhQVXjPjs"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FkLpfcRNSoFm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "from snntorch import spikegen\n",
        "from brian2 import *\n",
        "from brian2hears import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6vcXjzWStl0",
        "outputId": "cabef098-d834-4b3c-f3fe-53d19c0f4558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qXpXNVaSoFq"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OsF-UzANSoFr"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # Google Colab Path\n",
        "    \"metadata_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/metadata.parquet\",\n",
        "    \"ambisonics_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/ambisonics_sample\",\n",
        "    \"noise_ambisonics_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/noise_ambisonics_sample\",\n",
        "\n",
        "    # Local Path\n",
        "    # \"metadata_path\": r\"C:\\Users\\merca\\OneDrive\\Documents\\MyFiles\\Code\\Masters_Project\\spatial_librispeech_sample\\metadata.parquet\",\n",
        "    # \"ambisonics_path\": r\"c:\\Users\\merca\\OneDrive\\Documents\\MyFiles\\Code\\masters_project\\spatial_librispeech_sample\\ambisonics_sample\",\n",
        "\n",
        "    \"time_based_encoding\": True,\n",
        "    \"num_steps\": 20,\n",
        "    \"max_rate\": 10,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"noise\": True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOLJC3qwSoFs"
      },
      "source": [
        "# Filter Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ty5_0xH6wrFt"
      },
      "outputs": [],
      "source": [
        "def filter_data(metadata_path=config[\"metadata_path\"], ambisonics_path=config[\"ambisonics_path\"], noise_path=config[\"noise_ambisonics_path\"]):\n",
        "    metadata = pd.read_parquet(metadata_path, engine=\"pyarrow\")\n",
        "    ambisonics_files = [f for f in os.listdir(ambisonics_path) if os.path.isfile(os.path.join(ambisonics_path, f))]\n",
        "    noise_files = [f for f in os.listdir(noise_path) if os.path.isfile(os.path.join(noise_path, f))]\n",
        "\n",
        "\n",
        "    sample_ids = []\n",
        "\n",
        "    for file_name in ambisonics_files:\n",
        "        number, _ = file_name.split(\".\")\n",
        "        number.lstrip(\"0\")\n",
        "\n",
        "        if not number:\n",
        "            number = 0\n",
        "\n",
        "        sample_ids.append(int(number))\n",
        "\n",
        "    filtered_metadata = metadata[metadata[\"sample_id\"].isin(sample_ids)]\n",
        "\n",
        "    return filtered_metadata, ambisonics_files, noise_files\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toiYb9hzSoFv"
      },
      "source": [
        "# Preprocess Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E77Ry0ydSoFw"
      },
      "source": [
        "## Cochlear Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_K8G-G0FSoFw"
      },
      "outputs": [],
      "source": [
        "def cochlear_filter(channel_data, sr):\n",
        "    # Create a mono sound object from the channel data\n",
        "    sound = Sound(channel_data, samplerate=sr*Hz)\n",
        "\n",
        "    # Set up the cochlear model\n",
        "    num_channels = 32  # Number of channels in the filter bank\n",
        "    cf = erbspace(20*Hz, 20*kHz, num_channels)  # Center frequencies\n",
        "    gammatone = Gammatone(sound, cf)\n",
        "\n",
        "    # Process the sound with the cochlear filter\n",
        "    filtered_sound = gammatone.process()\n",
        "\n",
        "    # Convert the filtered signal to a numpy array\n",
        "    filtered_data = filtered_sound.T  # Transpose to get the correct shape\n",
        "    return filtered_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87GaxiccSoFw"
      },
      "source": [
        "## Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q8UrCaE0SoFx"
      },
      "outputs": [],
      "source": [
        "def normalize(data, device=config[\"device\"]):\n",
        "\n",
        "  if isinstance(data, np.ndarray):\n",
        "        data = torch.from_numpy(data).float()\n",
        "\n",
        "  # Move data to the specified device (GPU or CPU)\n",
        "  data = data.to(device)\n",
        "\n",
        "  return (data - data.min()) / (data.max() - data.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASBLdCnSoFx"
      },
      "source": [
        "## Rate Based Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2a5MyuVpSoFx"
      },
      "outputs": [],
      "source": [
        "def rate_based_encoding(data, max_rate=config[\"max_rate\"], num_steps=config[\"num_steps\"], device=config[\"device\"]):\n",
        "    if data is None:\n",
        "      raise ValueError(\"Input data is None.\")\n",
        "\n",
        "    data = torch.from_numpy(data).float().to(device)\n",
        "\n",
        "    normalized_data = normalize(data, device)\n",
        "\n",
        "    spike_rates = normalized_data * max_rate\n",
        "\n",
        "    spike_train = spikegen.rate(spike_rates, num_steps= num_steps)\n",
        "\n",
        "    return spike_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPJSC6NOSoFy"
      },
      "source": [
        "## Time Based Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qM_N4WhySoFy"
      },
      "outputs": [],
      "source": [
        "def time_based_encoding(data, num_steps=config[\"device\"], device=config[\"device\"]):\n",
        "    if data is None:\n",
        "      raise ValueError(\"Input data is None.\")\n",
        "\n",
        "    data_tensor = torch.from_numpy(data).float()\n",
        "\n",
        "    normalized_data = normalize(data_tensor)\n",
        "\n",
        "    spike_times = torch.where(normalized_data > 0.5, 1, 0)\n",
        "\n",
        "    spike_trains = spikegen.latency(spike_times, num_steps=num_steps)\n",
        "\n",
        "    return spike_trains\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBtjnet2x2Cb"
      },
      "source": [
        "## Preprocess Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PIk6i3YASoFy"
      },
      "outputs": [],
      "source": [
        "def preprocess_audio(ambisonic_filepath, noise_filepath ,max_duration):\n",
        "    \"\"\"\n",
        "    W: Omnidirectional\n",
        "    X: Front - Back\n",
        "    Y: Left - Right\n",
        "    Z: Top - Bottom\n",
        "    \"\"\"\n",
        "    audio, sr = librosa.load(ambisonic_filepath, sr=None, mono=False)\n",
        "    print(f\"Original shape: {audio.shape}, Sampling rate: {sr}\")\n",
        "\n",
        "    max_length = int(max_duration * sr)\n",
        "    print(f\"Max length in samples: {max_length}\")\n",
        "\n",
        "    padded_audio = librosa.util.fix_length(data=audio, size=max_length)\n",
        "\n",
        "\n",
        "    if config[\"noise\"] and noise_filepath:\n",
        "      noise_audio, _ = librosa.load(noise_filepath, sr=sr, mono=False)\n",
        "      padded_noise_audio = librosa.util.fix_length(data=noise_audio, size=max_length)\n",
        "\n",
        "      combined_audio = padded_audio + padded_noise_audio\n",
        "      print(f\"Speech and Noise have been combined\")\n",
        "    else:\n",
        "      combined_audio = padded_audio\n",
        "      print(f\"Speech and Noise have NOT been combined\")\n",
        "      print(f\"Padded shape: {combined_audio.shape}\")\n",
        "\n",
        "    # Process each channel separately\n",
        "    processed_W = cochlear_filter(combined_audio[0], sr)\n",
        "    processed_X = cochlear_filter(combined_audio[1], sr)\n",
        "    processed_Y = cochlear_filter(combined_audio[2], sr)\n",
        "    processed_Z = cochlear_filter(combined_audio[3], sr)\n",
        "    print(f\"processed_W.shape: {processed_W.shape}\")\n",
        "\n",
        "    if config[\"time_based_encoding\"]:\n",
        "        print(f\"Using Time Based Encoding\")\n",
        "        spike_trains_W = time_based_encoding(processed_W, config[\"num_steps\"])\n",
        "        spike_trains_X = time_based_encoding(processed_X, config[\"num_steps\"])\n",
        "        spike_trains_Y = time_based_encoding(processed_Y, config[\"num_steps\"])\n",
        "        spike_trains_Z = time_based_encoding(processed_Z, config[\"num_steps\"])\n",
        "    else:\n",
        "        print(f\"Using Rate Based Encoding\")\n",
        "        spike_trains_W = rate_based_encoding(processed_W, config['max_rate'], config['num_steps'])\n",
        "        spike_trains_X = rate_based_encoding(processed_X, config['max_rate'], config['num_steps'])\n",
        "        spike_trains_Y = rate_based_encoding(processed_Y, config['max_rate'], config['num_steps'])\n",
        "        spike_trains_Z = rate_based_encoding(processed_Z, config['max_rate'], config['num_steps'])\n",
        "\n",
        "    print(f\"spikes_W: {spike_trains_W.shape}\")\n",
        "    print(f\"spikes_X: {spike_trains_X.shape}\")\n",
        "    print(f\"spikes_Y: {spike_trains_Y.shape}\")\n",
        "    print(f\"spikes_Z: {spike_trains_Z.shape}\")\n",
        "\n",
        "    return spike_trains_W, spike_trains_X, spike_trains_Y, spike_trains_Z\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9lFyomXiH9I"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xIv-ITLhwrFv",
        "outputId": "6fe1e581-682a-4471-ebeb-72df50f5d176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (4, 251579), Sampling rate: 16000\n",
            "Max length in samples: 524310\n",
            "Speech and Noise have been combined\n",
            "processed_W.shape: (32, 524310)\n",
            "Using Time Based Encoding\n",
            "spikes_W: torch.Size([20, 32, 524310])\n",
            "spikes_X: torch.Size([20, 32, 524310])\n",
            "spikes_Y: torch.Size([20, 32, 524310])\n",
            "spikes_Z: torch.Size([20, 32, 524310])\n",
            "Original shape: (4, 244241), Sampling rate: 16000\n",
            "Max length in samples: 524310\n",
            "Speech and Noise have been combined\n",
            "processed_W.shape: (32, 524310)\n",
            "Using Time Based Encoding\n",
            "spikes_W: torch.Size([20, 32, 524310])\n",
            "spikes_X: torch.Size([20, 32, 524310])\n",
            "spikes_Y: torch.Size([20, 32, 524310])\n",
            "spikes_Z: torch.Size([20, 32, 524310])\n",
            "Original shape: (4, 327322), Sampling rate: 16000\n",
            "Max length in samples: 524310\n",
            "Speech and Noise have been combined\n",
            "processed_W.shape: (32, 524310)\n",
            "Using Time Based Encoding\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 15.77 GiB of which 138.38 MiB is free. Process 39535 has 15.63 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8bebb7e0c2dc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Process Each of The Files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mambisonic_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mprocessed_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processed_data is None. Check preprocess_audio Function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-556d6bea8659>\u001b[0m in \u001b[0;36mpreprocess_audio\u001b[0;34m(ambisonic_filepath, noise_filepath, max_duration)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mspike_trains_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_based_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_steps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mspike_trains_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_based_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_steps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mspike_trains_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_based_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_steps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mspike_trains_Z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_based_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_Z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_steps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-468dc61d0856>\u001b[0m in \u001b[0;36mtime_based_encoding\u001b[0;34m(data, num_steps, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mspike_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_data\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mspike_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspikegen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspike_trains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/spikegen.py\u001b[0m in \u001b[0;36mlatency\u001b[0;34m(data, num_steps, threshold, tau, first_spike_time, on_target, off_target, clip, normalize, linear, interpolate, bypass, epsilon)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mrm_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         spike_data = (\n\u001b[0;32m--> 310\u001b[0;31m             spike_data.scatter(\n\u001b[0m\u001b[1;32m    311\u001b[0m                 \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 15.77 GiB of which 138.38 MiB is free. Process 39535 has 15.63 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "filtered_metadata, ambisonics_files, noise_files = filter_data()\n",
        "max_duration = filtered_metadata[\"audio_info/duration\"].max()\n",
        "\n",
        "all_spike_trains_W = []\n",
        "all_spike_trains_X = []\n",
        "all_spike_trains_Y = []\n",
        "all_spike_trains_Z = []\n",
        "\n",
        "for ambisonic_file, noise_file in zip(ambisonics_files[:10], noise_files[:10]):\n",
        "  # Creat File Paths\n",
        "  ambisonic_file = os.path.join(config[\"ambisonics_path\"], ambisonic_file)\n",
        "  noise_file = os.path.join(config[\"noise_ambisonics_path\"], noise_file)\n",
        "\n",
        "  # Process Each of The Files\n",
        "  processed_data = preprocess_audio(ambisonic_file, noise_file, max_duration)\n",
        "  if processed_data is None:\n",
        "    raise ValueError(\"processed_data is None. Check preprocess_audio Function\")\n",
        "\n",
        "  spike_train_W, spike_train_X, spike_train_Y, spike_train_Z = processed_data\n",
        "\n",
        "  # Store Processed Data\n",
        "  all_spike_trains_W.append(spike_train_W)\n",
        "  all_spike_trains_X.append(spike_train_X)\n",
        "  all_spike_trains_Y.append(spike_train_Y)\n",
        "  all_spike_trains_Z.append(spike_train_Z)\n",
        "\n",
        "# Print Shape\n",
        "print(f\"all_spike_trains_W.shape: {all_spike_trains_W.shape}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}