{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mercadoerik1031/snn-sound-localization/blob/new_approach/snn_sound_localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwwANRnDjZcI"
      },
      "source": [
        "#**SNN Sounnd Localization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqx8gNURjTAG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6vcXjzWStl0",
        "outputId": "544c9e29-5edf-441e-decc-0dc80af2a450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf-z4LGrbj7m"
      },
      "source": [
        "# pip Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ue5M3A9bSHA",
        "outputId": "043da892-bb2b-4f08-f129-88f1702c1758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install icecream snntorch --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSMYypS0X3wv"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D2JtHkLFX47Z"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "\n",
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qXpXNVaSoFq"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OsF-UzANSoFr"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # Paths\n",
        "    \"metadata_path\": r\"/content/drive/My Drive/Colab Notebooks/Masters Project/metadata.parquet\",\n",
        "    \"ambisonic_path\": r\"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/ambisonics_sample\",\n",
        "    \"noise_path\": r\"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/noise_ambisonics_sample\",\n",
        "\n",
        "    # Device\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "\n",
        "    # Audio Info\n",
        "    \"sr\": 16_000,\n",
        "    \"n_fft\": 512,\n",
        "    \"hop_length\": 256,\n",
        "\n",
        "    # SNN\n",
        "    \"num_steps\": 20,\n",
        "\n",
        "    # DataLoader\n",
        "    \"batch_size\": 32,\n",
        "    \"seed\": 42,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxD1mqBjc2Rr"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LbtuY592ICC"
      },
      "source": [
        "## Load Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GRoEdDWwmsl5"
      },
      "outputs": [],
      "source": [
        "def filter_data(\n",
        "    metadata_path=config[\"metadata_path\"],\n",
        "    ambisonics_path=config[\"ambisonic_path\"],\n",
        "    noise_path=config[\"noise_path\"]\n",
        "    ):\n",
        "\n",
        "    metadata = pd.read_parquet(metadata_path, engine=\"pyarrow\")\n",
        "\n",
        "    # Get list of all files in directories\n",
        "    ambisonic_files = [f for f in os.listdir(ambisonics_path) if os.path.isfile(os.path.join(ambisonics_path, f))]\n",
        "    noise_files = [f for f in os.listdir(noise_path) if os.path.isfile(os.path.join(noise_path, f))]\n",
        "\n",
        "    sample_ids = [int(f.split(\".\")[0].lstrip(\"0\") or 0) for f in ambisonic_files]\n",
        "    filtered_metadata = metadata[metadata[\"sample_id\"].isin(sample_ids)]\n",
        "\n",
        "    # Create full file paths\n",
        "    ambisonic_files = [os.path.join(ambisonics_path, f) for f in ambisonic_files]\n",
        "    noise_files = [os.path.join(noise_path, f) for f in noise_files]\n",
        "\n",
        "    return filtered_metadata, ambisonic_files, noise_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwh3vT0uc36u"
      },
      "source": [
        "## Load and Pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UCSZ3nnKYFyI"
      },
      "outputs": [],
      "source": [
        "def load_and_pad(ambisonic_path, noise_path=None, sr=config[\"sr\"], max_duration=None):\n",
        "    if max_duration is None:\n",
        "        raise ValueError(\"Enter Value or max_duration\")\n",
        "\n",
        "    max_samples = max_duration * sr\n",
        "\n",
        "    ambi_audio, _ = librosa.load(ambisonic_path, sr=sr, mono=False)\n",
        "    ambi_audio = librosa.util.fix_length(ambi_audio, size=max_samples)\n",
        "\n",
        "    if noise_path:\n",
        "        noise_audio, _ = librosa.load(noise_path, sr=sr, mono=False)\n",
        "        noise_audio = librosa.util.fix_length(noise_audio, size=max_samples)\n",
        "        audio = ambi_audio + noise_audio\n",
        "\n",
        "    else:\n",
        "        audio = ambi_audio\n",
        "\n",
        "    return audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89EOIG0tRvc4"
      },
      "source": [
        "## Feature Extraction (STFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zJjdgzjZ2P9V"
      },
      "outputs": [],
      "source": [
        "def stft(audio, n_fft=config[\"n_fft\"], hop_length=config[\"hop_length\"]):\n",
        "    features = []\n",
        "\n",
        "    for i in range(audio.shape[0]):\n",
        "        channel = np.abs(librosa.stft(audio[i, :], n_fft=n_fft, hop_length=hop_length))\n",
        "        features.append(channel)\n",
        "\n",
        "    return np.array(features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY-QWVV5Rze1"
      },
      "source": [
        "## Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "v1F0rkT53VSK"
      },
      "outputs": [],
      "source": [
        "def normalize(features):\n",
        "    mean = np.mean(features, axis=0)\n",
        "    std = np.std(features, axis=0)\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    normalized_feat = (features - mean) / (std + epsilon)\n",
        "\n",
        "    return torch.tensor(normalized_feat).float()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlIOEPJGz780"
      },
      "source": [
        "## Split Metadata\n",
        "### Train, Val, Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XXuKphFYkqqp"
      },
      "outputs": [],
      "source": [
        "def split_data(metadata):\n",
        "    # Add a 'set' column to specify train, validation, or test\n",
        "    train_indices = metadata[metadata['split'] == 'train'].index\n",
        "    train_idx, valid_idx = train_test_split(train_indices, test_size=0.2, random_state=config[\"seed\"])\n",
        "\n",
        "    metadata['set'] = 'test'  # Initialize all as test\n",
        "    metadata.loc[train_idx, 'set'] = 'train'  # Mark train\n",
        "    metadata.loc[valid_idx, 'set'] = 'validation'  # Mark validation\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXArOgwz780"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "clGVHb3WezqQ"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, metadata, dataset_type, audio_files, noise_files=None, transform=None, sr=config[\"sr\"]):\n",
        "        # Filter metadata for the specified dataset type\n",
        "        self.metadata = metadata[metadata['set'] == dataset_type]\n",
        "        self.audio_files = audio_files\n",
        "        self.noise_files = noise_files\n",
        "        self.transform = transform\n",
        "        self.max_duration = int(np.ceil(self.metadata[\"audio_info/duration\"].max()))\n",
        "        self.sr = sr\n",
        "        self.num_steps = config[\"num_steps\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_files[idx]\n",
        "        noise_path = self.noise_files[idx] if self.noise_files else None\n",
        "\n",
        "        # Extract the sample id from the audio file name\n",
        "        sample_id = int(os.path.basename(audio_path).split(\".\")[0].lstrip(\"0\") or 0)\n",
        "\n",
        "        # Get the corresponding labels from the metadata\n",
        "        labels = self.metadata.loc[self.metadata['sample_id'] == sample_id, ['speech/azimuth', 'speech/elevation']].values[0]\n",
        "\n",
        "        audio = load_and_pad(audio_path, noise_path, self.sr, self.max_duration)\n",
        "        audio = stft(audio)\n",
        "        audio = normalize(audio)\n",
        "        audio = spikegen.rate(audio, self.num_steps)\n",
        "\n",
        "        if self.transform:\n",
        "            audio = self.transform(audio)\n",
        "\n",
        "        return audio, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzOF3CnFg_vu"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yrnNTfvmHeSI"
      },
      "outputs": [],
      "source": [
        "metadata, ambisonic_files, noise_files = filter_data()\n",
        "metadata = split_data(metadata)\n",
        "\n",
        "train_dataset = AudioDataset(metadata, \"train\", ambisonic_files[:33], noise_files[:33])\n",
        "valid_dataset = AudioDataset(metadata, \"validation\", ambisonic_files, noise_files)\n",
        "test_dataset = AudioDataset(metadata, \"test\", ambisonic_files, noise_files)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpibVGyYz_mm",
        "outputId": "d4b0f747-50c9-45dc-f166-d5a18b200ea6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| data.shape: torch.Size([20, 4, 257, 1063])\n",
            "ic| labels: array([2.0578608295207825, 0.03304128155335473], dtype=object)\n",
            "ic| data.shape: torch.Size([20, 4, 257, 1063])\n",
            "ic| labels: array([1.5422512964925659, 0.1091999980930245], dtype=object)\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, (data, labels) in enumerate(train_dataset):\n",
        "    ic(data.shape)\n",
        "    ic(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_3d_angle_error(true_azimuth, true_elevation, pred_azimuth, pred_elevation):\n",
        "    # Convert all angles from degrees to radians\n",
        "    true_azimuth = torch.deg2rad(true_azimuth)\n",
        "    true_elevation = torch.deg2rad(true_elevation)\n",
        "    pred_azimuth = torch.deg2rad(pred_azimuth)\n",
        "    pred_elevation = torch.deg2rad(pred_elevation)\n",
        "\n",
        "    # Calculate the 3D angle error\n",
        "    angle_error = torch.acos(\n",
        "        torch.sin(true_azimuth) * torch.sin(pred_azimuth) +\n",
        "        torch.cos(true_azimuth) * torch.cos(pred_azimuth) * torch.cos(true_elevation - pred_elevation)\n",
        "    )\n",
        "\n",
        "    # Convert the angle error back to degrees\n",
        "    angle_error = torch.rad2deg(angle_error)\n",
        "\n",
        "    # Calculate the median absolute error\n",
        "    median_abs_error = torch.median(torch.abs(angle_error))\n",
        "\n",
        "    return median_abs_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(SNN, self).__init__()\n",
        "        self.thresh1 = config[\"thresh1\"]\n",
        "        self.thresh2 = config[\"thresh2\"]\n",
        "        self.thresh3 = config[\"thresh3\"]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=(3, 3))\n",
        "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.lif1 = snn.Leaky(beta=self.beta, threshold=self.thresh1)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
        "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.lif2 = snn.Leaky(beta=self.beta, threshold=self.thresh2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3))\n",
        "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
        "        self.max_pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.lif3 = snn.Leaky(beta=self.beta, threshold=self.thresh3)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "        \n",
        "    def forward(self, inpt):\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        \n",
        "        out_rec = []\n",
        "        mem3_rec = []\n",
        "        \n",
        "        for step in range(self.num_steps):\n",
        "        \n",
        "            current1 = self.conv1(inpt)\n",
        "            current1 = self.batch_norm1(current1)\n",
        "            current1 = self.max_pool1(current1)\n",
        "            spike1, mem1 = self.lif1(current1, mem1)\n",
        "            \n",
        "            current2 = self.conv2(spike1)\n",
        "            current2 = self.batch_norm2(current2)\n",
        "            current2 = self.max_pool2(current2)\n",
        "            spike2, mem2 = self.lif2(current2, mem2)\n",
        "            \n",
        "            current3 = self.conv3(spike2)\n",
        "            current3 = self.batch_norm3(current3)\n",
        "            current3 = self.max_pool3(current3)\n",
        "            spike3, mem3 = self.lif3(current3, mem3)\n",
        "            \n",
        "            flatten = self.flatten(spike3)\n",
        "            fc = self.fc1(flatten)\n",
        "            out = self.fc2(fc)\n",
        "            \n",
        "            out_rec.append(out)\n",
        "            mem3_rec.append(mem3)\n",
        "        \n",
        "        return out\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = config[\"device\"]\n",
        "model = SNN(config).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        true_azimuths.append(labels[:, 0].detach())\n",
        "        true_elevations.append(labels[:, 1].detach())\n",
        "        pred_azimuths.append(outputs[:, 0].detach())\n",
        "        pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "    return train_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(model, valid_loader, criterion, device):\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    true_azimuths, true_elevations, pred_azimuths, pred_elevations = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(valid_loader):\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item() * data.size(0)\n",
        "            true_azimuths.append(labels[:, 0].detach())\n",
        "            true_elevations.append(labels[:, 1].detach())\n",
        "            pred_azimuths.append(outputs[:, 0].detach())\n",
        "            pred_elevations.append(outputs[:, 1].detach())\n",
        "\n",
        "    valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "    return valid_loss, torch.cat(true_azimuths), torch.cat(true_elevations), torch.cat(pred_azimuths), torch.cat(pred_elevations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(test_loader):\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * data.size(0)\n",
        "\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 10\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "    train_epoch_loss, train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation = train(model, train_loader, criterion, optimizer, device)\n",
        "    valid_epoch_loss, valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation = validate(model, valid_loader, criterion, device)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    valid_loss.append(valid_epoch_loss)\n",
        "    train_angle_error = calculate_3d_angle_error(train_true_azimuth, train_true_elevation, train_pred_azimuth, train_pred_elevation)\n",
        "    valid_angle_error = calculate_3d_angle_error(valid_true_azimuth, valid_true_elevation, valid_pred_azimuth, valid_pred_elevation)\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f} | Validation Loss: {valid_epoch_loss:.4f}\")\n",
        "    print(f\"Train Angle Error: {train_angle_error:.4f} | Validation Angle Error: {valid_angle_error:.4f}\")\n",
        "    \n",
        "test_loss = test(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3vmT8wWmkV1"
      },
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zUCDa6CFaZ29"
      },
      "outputs": [],
      "source": [
        "# W = audio[0]\n",
        "# X = audio[1]\n",
        "# Y = audio[2]\n",
        "# Z = audio[3]\n",
        "\n",
        "# W_n = audio_n[0]\n",
        "# Y_n = audio_n[2]\n",
        "# X_n = audio_n[1]\n",
        "# Z_n = audio_n[3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6U6m95pUcuLZ"
      },
      "outputs": [],
      "source": [
        "# # Plot each channel\n",
        "# fig, axs = plt.subplots(8, 1, figsize=(10, 8), sharex=True)\n",
        "\n",
        "# axs[0].plot(W)\n",
        "# axs[0].set_title('W Channel')\n",
        "# axs[1].plot(W_n)\n",
        "# axs[1].set_title('W_n Channel')\n",
        "\n",
        "# axs[2].plot(X)\n",
        "# axs[2].set_title('X Channel')\n",
        "# axs[3].plot(X_n)\n",
        "# axs[3].set_title('X_n Channel')\n",
        "\n",
        "\n",
        "# axs[4].plot(Y)\n",
        "# axs[4].set_title('Y Channel')\n",
        "# axs[5].plot(Y_n)\n",
        "# axs[5].set_title('Y_n Channel')\n",
        "\n",
        "# axs[6].plot(Z)\n",
        "# axs[6].set_title('Z Channel')\n",
        "# axs[7].plot(Z_n)\n",
        "# axs[7].set_title('Z_n Channel')\n",
        "\n",
        "# # Common settings for all subplots\n",
        "# for ax in axs:\n",
        "#     ax.set_ylabel('Amplitude')\n",
        "#     ax.label_outer()\n",
        "\n",
        "# axs[-1].set_xlabel('Sample')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "R3vmT8wWmkV1"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
