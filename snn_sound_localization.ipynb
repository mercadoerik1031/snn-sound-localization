{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mercadoerik1031/snn-sound-localization/blob/write_to_disk/snn_sound_localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwwANRnDjZcI"
      },
      "source": [
        "#**SNN Sounnd Localization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqx8gNURjTAG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E85Rvhujlsd"
      },
      "source": [
        "# Pip Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vmiMyIQVT8gh"
      },
      "outputs": [],
      "source": [
        "! pip install snntorch brian2 brian2hears --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vApVhQVXjPjs"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FkLpfcRNSoFm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from snntorch import spikegen\n",
        "from brian2 import *\n",
        "from brian2hears import *\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6vcXjzWStl0",
        "outputId": "9d9c16d3-1d5c-4fb2-9ad4-54199c687659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qXpXNVaSoFq"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OsF-UzANSoFr"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # Google Colab Path\n",
        "    \"metadata_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/metadata.parquet\",\n",
        "    \"ambisonics_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/ambisonics_sample\",\n",
        "    \"noise_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/noise_ambisonics_sample\",\n",
        "    \"output_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/preprocessed_samples\",\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"batch_size_pre\": 250,\n",
        "    \"sr\": 16000,\n",
        "\n",
        "    \"time_based_encoding\": True,\n",
        "    \"num_steps\": 10,\n",
        "    \"max_rate\": 10,\n",
        "    \"noise\": True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter Data"
      ],
      "metadata": {
        "id": "Y9C0Ea6NkSQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data(metadata_path=config[\"metadata_path\"], ambisonics_path=config[\"ambisonics_path\"], noise_path=config[\"noise_path\"]):\n",
        "  # Load metadata\n",
        "  metadata = pd.read_parquet(metadata_path, engine=\"pyarrow\")\n",
        "\n",
        "  # Get lists of all files in directories\n",
        "  ambisonic_files = [f for f in os.listdir(ambisonics_path) if os.path.isfile(os.path.join(ambisonics_path, f))]\n",
        "  noise_files = [f for f in os.listdir(noise_path) if os.path.isfile(os.path.join(noise_path, f))]\n",
        "\n",
        "  # Extract sample ids from filenames and filter metadata\n",
        "  sample_ids = [int(f.split(\".\")[0].lstrip(\"0\") or 0) for f in ambisonic_files]\n",
        "  filtered_metadata = metadata[metadata[\"sample_id\"].isin(sample_ids)]\n",
        "\n",
        "  # Create full file paths\n",
        "  ambisonic_files = [os.path.join(ambisonics_path, f) for f in ambisonic_files]\n",
        "  noise_files = [os.path.join(noise_path, f) for f in noise_files]\n",
        "\n",
        "  return filtered_metadata, ambisonic_files, noise_files\n"
      ],
      "metadata": {
        "id": "BRO_nYuFkUuK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toiYb9hzSoFv"
      },
      "source": [
        "# Preprocess Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cochlear Filter"
      ],
      "metadata": {
        "id": "_sSwdZRE1Ft7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def cochlear_filter(audio_data, sr):\n",
        "#   num_channels = audio_data.shape[0]\n",
        "#   processed_channels = []\n",
        "\n",
        "#   for channel in range(num_channels):\n",
        "#     sound = Sound(audio_data[channel], samplerate=sr*Hz)\n",
        "#     cf = erbspace(20*Hz, 20*kHz, 32)\n",
        "#     gammatone = Gammatone(sound, cf)\n",
        "#     filtered_sound = gammatone.process()\n",
        "#     filtered_data = filtered_sound.T\n",
        "#     processed_channels.append(filtered_data)\n",
        "\n",
        "#   combined_processed_data = np.stack(processed_channels, axis=0)\n",
        "#   return combined_processed_data\n"
      ],
      "metadata": {
        "id": "5Uvz_iZwzNs_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87GaxiccSoFw"
      },
      "source": [
        "## Normalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(audio_data, device=config[\"device\"]):\n",
        "  audio_data = audio_data.to(device)\n",
        "  return (audio_data - audio_data.min()) / (audio_data.max() - audio_data.min())\n"
      ],
      "metadata": {
        "id": "615Sk9vy0GER"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASBLdCnSoFx"
      },
      "source": [
        "## Rate Based Encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_based_encoding(audio_data, max_rate=config[\"max_rate\"], num_steps=config[\"num_steps\"], device=config[\"device\"]):\n",
        "    if audio_data is None:\n",
        "        raise ValueError(\"Input data is None.\")\n",
        "\n",
        "    # Check if audio_data is already a tensor, if not convert it\n",
        "    if not isinstance(audio_data, torch.Tensor):\n",
        "        audio_data = torch.tensor(audio_data, device=device)\n",
        "\n",
        "    audio_data = audio_data.float().to(device)\n",
        "\n",
        "    normalized_data = normalize(audio_data)\n",
        "\n",
        "    spike_rates = normalized_data * max_rate\n",
        "\n",
        "    spike_train = spikegen.rate(spike_rates, num_steps=num_steps)\n",
        "\n",
        "    return spike_train\n",
        "\n"
      ],
      "metadata": {
        "id": "0WkaWpuW0hzE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPJSC6NOSoFy"
      },
      "source": [
        "## Time Based Encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_based_encoding(audio_data, num_steps=config[\"num_steps\"], device=config[\"device\"]):\n",
        "    if audio_data is None:\n",
        "        raise ValueError(\"Input data is None.\")\n",
        "\n",
        "    # Check if audio_data is already a tensor, if not convert it\n",
        "    if not isinstance(audio_data, torch.Tensor):\n",
        "        audio_data = torch.tensor(audio_data, device=device)\n",
        "\n",
        "    audio_data = audio_data.float().to(device)\n",
        "\n",
        "    normalized_data = normalize(audio_data)\n",
        "\n",
        "    spike_times = torch.where(normalized_data > 0.5, 1, 0)\n",
        "\n",
        "    spike_train = spikegen.latency(spike_times, num_steps=num_steps, bypass=True)\n",
        "\n",
        "    return spike_train\n"
      ],
      "metadata": {
        "id": "qkLw8tJk0lRx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Function"
      ],
      "metadata": {
        "id": "uCy0L63q5ReB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(ambisonic_file, noise_file, duration, device=config[\"device\"], sr=config[\"sr\"]):\n",
        "    # Load ambisonic audio directly to GPU if possible\n",
        "    audio = torch.tensor(librosa.load(ambisonic_file, sr=sr, mono=False, duration=duration)[0], device=device)\n",
        "    length = int(np.round(duration * sr))\n",
        "\n",
        "    # Pad Ambisonic File\n",
        "    padded_ambisonic = torch.nn.functional.pad(audio, (0, max(0, length - audio.shape[1])))\n",
        "\n",
        "    # Combine Noise (Optional)\n",
        "    if config[\"noise\"] and noise_file:\n",
        "        # Load Noise File\n",
        "        noise_audio = torch.tensor(librosa.load(noise_file, sr=sr, mono=False, duration=duration)[0], device=device)\n",
        "\n",
        "        # Pad Noise File\n",
        "        padded_noise = torch.nn.functional.pad(noise_audio, (0, max(0, length - noise_audio.shape[1])))\n",
        "\n",
        "        # Combine Ambisonic & Noise\n",
        "        combined_audio = padded_ambisonic + padded_noise\n",
        "    else:\n",
        "        combined_audio = padded_ambisonic\n",
        "\n",
        "    # Processed_audio should be processed on GPU\n",
        "    spike_trains = time_based_encoding(combined_audio) if config[\"time_based_encoding\"] else rate_based_encoding(combined_audio)\n",
        "\n",
        "    return spike_trains\n",
        "\n"
      ],
      "metadata": {
        "id": "LPJVTkMLBqYM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Append to File"
      ],
      "metadata": {
        "id": "6jDKDbzmphC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def append_to_file(data, filepath):\n",
        "#   # Append data to an existing file or create a new one\n",
        "#   if os.path.exists(filepath):\n",
        "#       existing_data = torch.load(filepath)\n",
        "#       combined_data = torch.cat((existing_data, data), dim=0)\n",
        "#       torch.save(combined_data, filepath)\n",
        "#   else:\n",
        "#       torch.save(data, filepath)"
      ],
      "metadata": {
        "id": "qQLcAW8rpgdx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_batches(batch_files, output_path):\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "\n",
        "    for data_file, label_file in batch_files:\n",
        "        # Load processed data and labels\n",
        "        batch_data = torch.load(os.path.join(output_path, data_file))\n",
        "        batch_labels = pd.read_csv(os.path.join(output_path, label_file))\n",
        "\n",
        "        # Append to the consolidated lists\n",
        "        all_data.append(batch_data)\n",
        "        all_labels.append(batch_labels)\n",
        "\n",
        "        # Optionally delete the individual batch files\n",
        "        os.remove(os.path.join(output_path, data_file))\n",
        "        os.remove(os.path.join(output_path, label_file))\n",
        "\n",
        "    # Save the consolidated data and labels\n",
        "    torch.save(torch.cat(all_data), os.path.join(output_path, 'consolidated_processed_data.pt'))\n",
        "    pd.concat(all_labels, ignore_index=True).to_csv(os.path.join(output_path, 'consolidated_labels.csv'), index=False)\n",
        "    print(\"All data consolidated and saved.\")"
      ],
      "metadata": {
        "id": "CpJF3DPOyOtc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process & Save Batches"
      ],
      "metadata": {
        "id": "fQAlzqvk13al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batches(metadata, ambisonic_files, noise_files, duration, batch_size=config[\"batch_size_pre\"], output_path=config[\"output_path\"], sr=config[\"sr\"]):\n",
        "    batch_files = []  # List to store the names of batch files\n",
        "\n",
        "    # Process each batch\n",
        "    for i in range(0, len(ambisonic_files), batch_size):\n",
        "        print(f\"Processing batch {i} to {i+batch_size}\")\n",
        "        batch_ambisonic_files = ambisonic_files[i:i+batch_size]\n",
        "        batch_noise_files = noise_files[i:i+batch_size]\n",
        "        batch_metadata = metadata.iloc[i:i+batch_size]\n",
        "\n",
        "        processed_data = []\n",
        "        labels = []\n",
        "\n",
        "        # Process each file in the batch\n",
        "        for ambisonic_file, noise_file, meta_row in zip(batch_ambisonic_files, batch_noise_files, batch_metadata.itertuples()):\n",
        "            spike_trains = preprocess(ambisonic_file, noise_file, duration)\n",
        "            processed_data.append(spike_trains.cpu())\n",
        "\n",
        "            labels.append({\n",
        "                'sample_id': meta_row.sample_id,\n",
        "                'split': meta_row.split,\n",
        "                'azimuth': batch_metadata.at[meta_row.Index, 'speech/azimuth'],\n",
        "                'elevation': batch_metadata.at[meta_row.Index, 'speech/elevation']\n",
        "            })\n",
        "\n",
        "        # Save processed data and labels for the batch\n",
        "        batch_data_filename = f'processed_batch_{i}.pt'\n",
        "        batch_labels_filename = f'labels_batch_{i}.csv'\n",
        "        torch.save(torch.stack(processed_data), os.path.join(output_path, batch_data_filename))\n",
        "        pd.DataFrame(labels).to_csv(os.path.join(output_path, batch_labels_filename), index=False)\n",
        "        batch_files.append((batch_data_filename, batch_labels_filename))\n",
        "\n",
        "        # Clear memory\n",
        "        del processed_data, labels\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"Batch {i} to {i+batch_size} processed and saved.\")\n",
        "\n",
        "    # Consolidate all batch files\n",
        "    consolidate_batches(batch_files, output_path)\n"
      ],
      "metadata": {
        "id": "MM4wTQST1668"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_metadata, ambisonic_files, noise_files = filter_data()\n",
        "duration = filtered_metadata[\"audio_info/duration\"].mean() + filtered_metadata[\"audio_info/duration\"].std()\n",
        "process_batches(filtered_metadata, ambisonic_files, noise_files, duration)"
      ],
      "metadata": {
        "id": "zrjIlu7a33yd",
        "outputId": "b6d5cc51-41bf-4125-afcc-04ceb9d7523e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 0 to 250\n",
            "Batch 0 to 250 processed and saved.\n",
            "Processing batch 250 to 500\n",
            "Batch 250 to 500 processed and saved.\n",
            "Processing batch 500 to 750\n",
            "Batch 500 to 750 processed and saved.\n",
            "All data consolidated and saved.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}