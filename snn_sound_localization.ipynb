{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mercadoerik1031/snn-sound-localization/blob/main/snn_sound_localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwwANRnDjZcI"
      },
      "source": [
        "#**SNN Sounnd Localization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqx8gNURjTAG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E85Rvhujlsd"
      },
      "source": [
        "# Pip Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vmiMyIQVT8gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12780595-4d03-4305-ba47-e2a762c0c20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for brian2hears (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install snntorch brian2 brian2hears --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vApVhQVXjPjs"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FkLpfcRNSoFm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "from snntorch import spikegen\n",
        "from brian2 import *\n",
        "from brian2hears import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6vcXjzWStl0",
        "outputId": "d752698e-aca1-481c-dfa3-eadf93f9b5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qXpXNVaSoFq"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OsF-UzANSoFr"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # Google Colab Path\n",
        "    \"metadata_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/metadata.parquet\",\n",
        "    \"ambisonics_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/ambisonics_sample\",\n",
        "    \"noise_ambisonics_path\": \"/content/drive/My Drive/Colab Notebooks/Masters Project/spatial_librispeech_sample/noise_ambisonics_sample\",\n",
        "\n",
        "    # Local Path\n",
        "    # \"metadata_path\": r\"C:\\Users\\merca\\OneDrive\\Documents\\MyFiles\\Code\\Masters_Project\\spatial_librispeech_sample\\metadata.parquet\",\n",
        "    # \"ambisonics_path\": r\"c:\\Users\\merca\\OneDrive\\Documents\\MyFiles\\Code\\masters_project\\spatial_librispeech_sample\\ambisonics_sample\",\n",
        "\n",
        "    \"time_based_encoding\": True,\n",
        "    \"num_steps\": 10,\n",
        "    \"max_rate\": 10,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"noise\": True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOLJC3qwSoFs"
      },
      "source": [
        "# Filter Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ty5_0xH6wrFt"
      },
      "outputs": [],
      "source": [
        "def filter_data(metadata_path=config[\"metadata_path\"], ambisonics_path=config[\"ambisonics_path\"], noise_path=config[\"noise_ambisonics_path\"]):\n",
        "\n",
        "    metadata = pd.read_parquet(metadata_path, engine=\"pyarrow\")\n",
        "    ambisonics_files = [f for f in os.listdir(ambisonics_path) if os.path.isfile(os.path.join(ambisonics_path, f))]\n",
        "    noise_files = [f for f in os.listdir(noise_path) if os.path.isfile(os.path.join(noise_path, f))]\n",
        "\n",
        "    sample_ids = []\n",
        "\n",
        "    for file_name in ambisonics_files:\n",
        "        number, _ = file_name.split(\".\")\n",
        "        number.lstrip(\"0\")\n",
        "\n",
        "        if not number:\n",
        "            number = 0\n",
        "\n",
        "        sample_ids.append(int(number))\n",
        "\n",
        "    filtered_metadata = metadata[metadata[\"sample_id\"].isin(sample_ids)]\n",
        "\n",
        "    return filtered_metadata, ambisonics_files, noise_files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_metadata, ambisonics_files, noise_files = filter_data()"
      ],
      "metadata": {
        "id": "_6LTpL2H6jHK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Max: {filtered_metadata['audio_info/duration'].max()}\")\n",
        "print(f\"Mean: {filtered_metadata['audio_info/duration'].mean()}\")\n",
        "print(f\"Min: {filtered_metadata['audio_info/duration'].min()}\")\n",
        "print(f\"Median: {filtered_metadata['audio_info/duration'].median()}\")\n",
        "print(f\"STD: {filtered_metadata['audio_info/duration'].std()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPcjmxwu7AsU",
        "outputId": "2e570f6e-94b8-468b-c4cc-faddc6f1ee03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max: 32.769375\n",
            "Mean: 10.773610714285715\n",
            "Min: 0.71825\n",
            "Median: 12.628875\n",
            "STD: 4.748215274235851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toiYb9hzSoFv"
      },
      "source": [
        "# Preprocess Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E77Ry0ydSoFw"
      },
      "source": [
        "## Cochlear Filter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cochlear_filter(audio_data, sr):\n",
        "    num_channels = audio_data.shape[0]  # Number of channels in the audio data\n",
        "    processed_channels = []\n",
        "\n",
        "    for channel in range(num_channels):\n",
        "        sound = Sound(audio_data[channel], samplerate=sr*Hz)\n",
        "        cf = erbspace(20*Hz, 20*kHz, 32)  # Center frequencies for 32 channels\n",
        "        gammatone = Gammatone(sound, cf)\n",
        "        filtered_sound = gammatone.process()\n",
        "        filtered_data = filtered_sound.T  # Transpose to get the correct shape\n",
        "        processed_channels.append(filtered_data)\n",
        "\n",
        "    # Combine the processed data from all channels\n",
        "    combined_processed_data = np.stack(processed_channels, axis=0)\n",
        "    return combined_processed_data\n"
      ],
      "metadata": {
        "id": "P7SC0QoxILG9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def cochlear_filter(audio_data, st):\n",
        "#   return audio_data"
      ],
      "metadata": {
        "id": "UmYVCH2_eeVq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87GaxiccSoFw"
      },
      "source": [
        "## Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q8UrCaE0SoFx"
      },
      "outputs": [],
      "source": [
        "def normalize(data, device=config[\"device\"]):\n",
        "\n",
        "  if isinstance(data, np.ndarray):\n",
        "        data = torch.from_numpy(data).float()\n",
        "\n",
        "  # Move data to the specified device (GPU or CPU)\n",
        "  data = data.to(device)\n",
        "\n",
        "  return (data - data.min()) / (data.max() - data.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASBLdCnSoFx"
      },
      "source": [
        "## Rate Based Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2a5MyuVpSoFx"
      },
      "outputs": [],
      "source": [
        "def rate_based_encoding(data, max_rate=config[\"max_rate\"], num_steps=config[\"num_steps\"], device=config[\"device\"]):\n",
        "    if data is None:\n",
        "      raise ValueError(\"Input data is None.\")\n",
        "\n",
        "    data = torch.from_numpy(data).float().to(device)\n",
        "\n",
        "    normalized_data = normalize(data, device)\n",
        "\n",
        "    spike_rates = normalized_data * max_rate\n",
        "\n",
        "    spike_train = spikegen.rate(spike_rates, num_steps=num_steps)\n",
        "\n",
        "    return spike_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPJSC6NOSoFy"
      },
      "source": [
        "## Time Based Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qM_N4WhySoFy"
      },
      "outputs": [],
      "source": [
        "def time_based_encoding(data, num_steps=config[\"device\"], device=config[\"device\"]):\n",
        "    if data is None:\n",
        "      raise ValueError(\"Input data is None.\")\n",
        "\n",
        "    data_tensor = torch.from_numpy(data).float()\n",
        "\n",
        "    normalized_data = normalize(data_tensor)\n",
        "\n",
        "    spike_times = torch.where(normalized_data > 0.5, 1, 0)\n",
        "\n",
        "    spike_train = spikegen.latency(spike_times, num_steps=num_steps, bypass=True)\n",
        "\n",
        "    print(f\"spike_train.shape: {spike_train.shape}\")\n",
        "\n",
        "    return spike_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBtjnet2x2Cb"
      },
      "source": [
        "## Preprocess Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PIk6i3YASoFy"
      },
      "outputs": [],
      "source": [
        "def preprocess_audio(ambisonic_filepath, noise_filepath, start_time, end_time, sr):\n",
        "    duration = end_time - start_time\n",
        "    max_length = int(duration * sr)\n",
        "\n",
        "    # Load ambisonic audio\n",
        "    audio, _ = librosa.load(ambisonic_filepath, sr=sr, mono=False, offset=start_time, duration=duration)\n",
        "    audio = audio if audio.ndim == 2 else np.expand_dims(audio, axis=0)  # Ensure audio is 2D\n",
        "    padded_audio = librosa.util.fix_length(audio, size=max_length, axis=1)\n",
        "\n",
        "    if config[\"noise\"] and noise_filepath:\n",
        "        # Load noise audio\n",
        "        noise_audio, _ = librosa.load(noise_filepath, sr=sr, mono=False, offset=start_time, duration=duration)\n",
        "        noise_audio = noise_audio if noise_audio.ndim == 2 else np.expand_dims(noise_audio, axis=0)  # Ensure noise audio is 2D\n",
        "        padded_noise_audio = librosa.util.fix_length(noise_audio, size=max_length, axis=1)\n",
        "\n",
        "        # Ensure the channel counts are equal\n",
        "        min_channels = min(padded_audio.shape[0], padded_noise_audio.shape[0])\n",
        "        combined_audio = padded_audio[:min_channels] + padded_noise_audio[:min_channels]\n",
        "    else:\n",
        "        combined_audio = padded_audio\n",
        "\n",
        "    processed_audio = cochlear_filter(combined_audio, sr)\n",
        "\n",
        "    # Encoding\n",
        "    if config[\"time_based_encoding\"]:\n",
        "        spike_trains = time_based_encoding(processed_audio, config[\"num_steps\"])\n",
        "    else:\n",
        "        spike_trains = rate_based_encoding(processed_audio, config['max_rate'], config[\"num_steps\"])\n",
        "\n",
        "    return spike_trains\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blocks"
      ],
      "metadata": {
        "id": "ZhJDmLM4iGNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio_in_blocks(block_duration, sr):\n",
        "    ambisonics_path = config[\"ambisonics_path\"]\n",
        "    noise_ambisonics_path = config[\"noise_ambisonics_path\"]\n",
        "\n",
        "    ambisonics_files = sorted([f for f in os.listdir(ambisonics_path) if os.path.isfile(os.path.join(ambisonics_path, f))])\n",
        "    noise_files = sorted([f for f in os.listdir(noise_ambisonics_path) if os.path.isfile(os.path.join(noise_ambisonics_path, f))])\n",
        "\n",
        "    all_processed_data = []\n",
        "\n",
        "    for ambisonic_file, noise_file in zip(ambisonics_files, noise_files):\n",
        "      print(f\"ambisonic_file: {ambisonic_file}. noise_file: {noise_file}\")\n",
        "        ambisonic_filepath = os.path.join(ambisonics_path, ambisonic_file)\n",
        "        noise_filepath = os.path.join(noise_ambisonics_path, noise_file)\n",
        "\n",
        "        # Get the duration of the audio file (assuming both ambisonic and noise files have the same duration)\n",
        "        total_duration = librosa.get_duration(path=ambisonic_filepath, sr=sr)\n",
        "        total_blocks = int(total_duration // block_duration) + (total_duration % block_duration > 0)\n",
        "\n",
        "        for block_idx in range(total_blocks):\n",
        "            start_time = block_idx * block_duration\n",
        "            end_time = min((block_idx + 1) * block_duration, total_duration)\n",
        "\n",
        "            processed_block = preprocess_audio(ambisonic_filepath, noise_filepath, start_time, end_time, sr)\n",
        "            all_processed_data.append(processed_block)\n",
        "\n",
        "    final_preprocessed_data = torch.stack(all_processed_data)\n",
        "\n",
        "    return final_preprocessed_data\n"
      ],
      "metadata": {
        "id": "q5qUZrxCiFih"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9lFyomXiH9I"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_duration = 5.0  # Duration of each block in seconds\n",
        "sample_rate = 16000  # Sample rate for audio files\n",
        "\n",
        "preprocessed_data = preprocess_audio_in_blocks(\n",
        "    block_duration=block_duration,\n",
        "    sr=sample_rate\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Rex8cGVnZjZ",
        "outputId": "d34e7256-be82-4fc4-c3d0-7a8fe7074ab7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 71352])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 69368])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 56428])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 27578])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 78180])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 38574])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 62973])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 62757])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 19782])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 21711])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 65889])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 31377])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 43304])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 44944])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 68278])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n",
            "spike_train.shape: torch.Size([10, 4, 32, 80000])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 59.06 MiB is free. Process 2088 has 14.69 GiB memory in use. Of the allocated memory 13.70 GiB is allocated by PyTorch, and 897.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9a6c4561c428>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m  \u001b[0;31m# Sample rate for audio files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m preprocessed_data = preprocess_audio_in_blocks(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mblock_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock_duration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3d939a0c8a72>\u001b[0m in \u001b[0;36mpreprocess_audio_in_blocks\u001b[0;34m(block_duration, sr)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mblock_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mprocessed_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mambisonic_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mall_processed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e4a34e88e6fd>\u001b[0m in \u001b[0;36mpreprocess_audio\u001b[0;34m(ambisonic_filepath, noise_filepath, start_time, end_time, sr)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_based_encoding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mspike_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_based_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_steps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mspike_trains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate_based_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_steps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1937de9d45b7>\u001b[0m in \u001b[0;36mtime_based_encoding\u001b[0;34m(data, num_steps, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mspike_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_data\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mspike_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspikegen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbypass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"spike_train.shape: {spike_train.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/snntorch/spikegen.py\u001b[0m in \u001b[0;36mlatency\u001b[0;34m(data, num_steps, threshold, tau, first_spike_time, on_target, off_target, clip, normalize, linear, interpolate, bypass, epsilon)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mrm_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         spike_data = (\n\u001b[0;32m--> 310\u001b[0;31m             spike_data.scatter(\n\u001b[0m\u001b[1;32m    311\u001b[0m                 \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 59.06 MiB is free. Process 2088 has 14.69 GiB memory in use. Of the allocated memory 13.70 GiB is allocated by PyTorch, and 897.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# _, ambisonics_files, noise_files = filter_data()\n",
        "# batch_size = 10  # Adjust this based on your memory capacity\n",
        "# total_files = len(ambisonics_files)\n",
        "# batches = total_files // batch_size\n",
        "# num_steps = 10\n",
        "# num_channels = 4\n",
        "# sr = 16_000\n",
        "# duration = 5\n",
        "# length_samples = duration * sr\n",
        "# num_cochlear_filters = 32\n",
        "\n",
        "# for batch_idx in range(batches):\n",
        "#     start_idx = batch_idx * batch_size\n",
        "#     end_idx = start_idx + batch_size\n",
        "\n",
        "#     # Adjust the tensor size for the current batch\n",
        "#     current_batch_size = min(batch_size, total_files - start_idx)\n",
        "#     all_processed_data_batch = torch.empty(current_batch_size, num_steps, num_channels, num_cochlear_filters, length_samples, device=config[\"device\"])\n",
        "#     # all_processed_data_batch = torch.empty(current_batch_size, num_steps, num_channels, length_samples, device=config[\"device\"])\n",
        "\n",
        "#     for idx, (ambisonic_file, noise_file) in enumerate(zip(ambisonics_files[start_idx:end_idx], noise_files[start_idx:end_idx])):\n",
        "#         # Create file paths\n",
        "#         ambisonic_file = os.path.join(config[\"ambisonics_path\"], ambisonic_file)\n",
        "#         noise_file = os.path.join(config[\"noise_ambisonics_path\"], noise_file)\n",
        "\n",
        "#         # Process each file\n",
        "#         processed_data = preprocess_audio(ambisonic_file, noise_file, duration)\n",
        "#         if processed_data is None:\n",
        "#             raise ValueError(\"processed_data is None. Check preprocess_audio Function\")\n",
        "\n",
        "#         # Store the processed data in the preallocated tensor for the batch\n",
        "#         all_processed_data_batch[idx] = processed_data\n",
        "\n",
        "#     # Now all_processed_data_batch contains the processed data for the current batch\n",
        "#     # Use this data for training your model\n",
        "#     # train_your_model(all_processed_data_batch)\n",
        "\n",
        "#     # After training, clear the memory\n",
        "#     del all_processed_data_batch\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "# # If there are remaining files that were not processed (due to uneven division), process them here\n",
        "# # This step is optional and only needed if total_files is not a multiple of batch_size\n",
        "# remaining_files = total_files % batch_size\n",
        "# if remaining_files > 0:\n",
        "#     start_idx = batches * batch_size\n",
        "#     end_idx = total_files\n",
        "#     all_processed_data_remaining = torch.empty(remaining_files, num_steps, num_channels, num_cochlear_filters, length_samples, device=config[\"device\"])\n",
        "#     # all_processed_data_remaining = torch.empty(remaining_files, num_steps, num_channels, length_samples, device=config[\"device\"])\n",
        "\n",
        "#     # for idx, (ambisonic_file, noise_file) in enumerate(zip(ambisonics_files[start_idx:end_idx], noise_files[start_idx:end_idx])):\n",
        "#     #     # ... (process each file and store in all_processed_data_remaining)\n",
        "\n",
        "#     # # Use all_processed_data_remaining for training, then discard it\n",
        "#     # # ...\n",
        "\n",
        "#     del all_processed_data_remaining\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "# # Note: Replace 'train_your_model()' with the actual function that trains your model"
      ],
      "metadata": {
        "id": "JtB78b-FgPI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtered_metadata, ambisonics_files, noise_files = filter_data()\n",
        "\n",
        "# # Parameters\n",
        "# num_files = len(ambisonics_files)  # Number of files to process\n",
        "# num_steps = config[\"num_steps\"]\n",
        "# num_channels = 4  # Number of channels in ambisonic audio\n",
        "# num_cochlear_filters = 32\n",
        "# sr = 16000  # Sampling rate\n",
        "# duration = 5  # Maximum duration in seconds\n",
        "# length_samples = duration * sr\n",
        "\n",
        "# # Preallocate tensor to store all processed data\n",
        "# # all_processed_data = torch.empty(num_files, num_steps, num_channels, num_cochlear_filters, length_samples, device=config[\"device\"])\n",
        "# all_processed_data = torch.empty(num_files, num_steps, num_channels, length_samples, device=config[\"device\"])\n",
        "\n",
        "# for idx, (ambisonic_file, noise_file) in enumerate(zip(ambisonics_files, noise_files)):\n",
        "#     # Create file paths\n",
        "#     ambisonic_file = os.path.join(config[\"ambisonics_path\"], ambisonic_file)\n",
        "#     noise_file = os.path.join(config[\"noise_ambisonics_path\"], noise_file)\n",
        "\n",
        "#     # Process each of the files\n",
        "#     processed_data = preprocess_audio(ambisonic_file, noise_file, duration)\n",
        "#     if processed_data is None:\n",
        "#         raise ValueError(\"processed_data is None. Check preprocess_audio Function\")\n",
        "\n",
        "#     # Store the processed data in the preallocated tensor\n",
        "#     all_processed_data[idx] = processed_data\n",
        "\n",
        "# # Print Shape\n",
        "# print(f\"all_processed_data.shape: {all_processed_data.shape}\")\n"
      ],
      "metadata": {
        "id": "u94Gtvk9Hey9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}